<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<title>XGBoost调参详解</title>
<link href="https://reed-qu.github.io/theme/css/main.css" rel="stylesheet"/>
<link href="https://reed-qu.github.io/feeds/all.atom.xml" rel="alternate" title="[reed@blog ~]# _ Atom Feed" type="application/atom+xml"/>
</head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://reed-qu.github.io/">[reed@blog ~]# _ </a></h1>
<nav><ul>
<li class="active"><a href="https://reed-qu.github.io/category/machine-learning.html">machine learning</a></li>
</ul></nav>
</header><!-- /#banner -->
<section class="body" id="content">
<article>
<header>
<h1 class="entry-title">
<a href="https://reed-qu.github.io/xgboostdiao-can-xiang-jie.html" rel="bookmark" title="Permalink to XGBoost调参详解">XGBoost调参详解</a></h1>
</header>
<div class="entry-content">
<footer class="post-info">
<abbr class="published" title="2020-01-24T22:20:14+08:00">
                Published: 2020-01-24
        </abbr>
<address class="vcard author">
                By                         <a class="url fn" href="https://reed-qu.github.io/author/reed.html">reed</a>
</address>
<p>In <a href="https://reed-qu.github.io/category/machine-learning.html">machine learning</a>.</p>
</footer><!-- /.post-info --> <blockquote>
<p>在之前的一篇文章中，从 <strong>GBDT</strong> 一直说到当下最流行的梯度提升树模型之一 <strong><a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"><em>XGBoost</em></a></strong>，今天这里主要说应用XGB这个算法包的一些参数问题，在实际应用中，我们并不会自己动手去实现一个XGB，了解更多的XGB的算法原理，也是为了我们在工程实践中更好的应对各种问题，了解每一个参数背后的意义，有助我们训练出来更好的模型</p>
</blockquote>
<h2><strong>普通参数 [General Parameters]</strong></h2>
<div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">.</span> <span class="n">booster</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="n">gbtree</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">表示应用的弱学习器的类型</span><span class="p">,</span> <span class="err">推荐用默认参数</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">可选的有</span><span class="n">gbtree</span><span class="p">,</span> <span class="n">dart</span><span class="p">,</span> <span class="n">gblinear</span>
        <span class="n">gblinear是线性模型</span><span class="err">，表现很差，接近一个</span><span class="n">LASSO</span>
        <span class="n">dart是树模型的一种</span><span class="err">，思想是每次训练新树的时候，随机从前</span><span class="n">m轮的树中扔掉一些</span><span class="err">，来避免过拟合</span>
        <span class="n">gbtree即是论文中主要讨论的树模型</span><span class="err">，推荐使用</span>

<span class="mi">2</span><span class="p">.</span> <span class="n">silent</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="err">不推荐</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">不推荐使用，推荐使用</span><span class="n">verbosity参数来代替</span><span class="err">，功能更强大</span>

<span class="mi">3</span><span class="p">.</span> <span class="n">verbosity</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">训练过程中打印的日志等级，</span><span class="mi">0</span> <span class="p">(</span><span class="n">silent</span><span class="p">),</span> <span class="mi">1</span> <span class="p">(</span><span class="n">warning</span><span class="p">),</span> <span class="mi">2</span> <span class="p">(</span><span class="n">info</span><span class="p">),</span> <span class="mi">3</span> <span class="p">(</span><span class="n">debug</span><span class="p">)</span>

<span class="mi">4</span><span class="p">.</span> <span class="n">nthread</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="err">最大可用线程数</span><span class="p">][</span><span class="k">alias</span><span class="p">:</span> <span class="n">n_jobs</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">训练过程中的并行线程数</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">如果用的是</span><span class="n">sklearn的api</span><span class="err">，那么使用</span><span class="n">n_jobs来代替</span>
</pre></div>
<h2><strong>每个分类器算法参数 [Booster Parameters]</strong></h2>
<div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">.</span> <span class="n">eta</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">]</span> <span class="p">[</span><span class="k">alias</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">就是常说的学习速率，控制每一次学习的权重缩减，给后来的模型提供更多的学习空间</span>

<span class="mi">2</span><span class="p">.</span> <span class="n">gamma</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="k">alias</span><span class="p">:</span> <span class="n">min_split_loss</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">叶子节点分裂时所需要的最小的损失减少量，这个值越大，叶子节点越难分裂，所以算法就越保守</span>
</pre></div>
<p><strong><em>gamma</em></strong> (  <span class="math">\( \gamma\)</span>  )就是 <a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"> 从GBDT到XGBoost</a> 中的正则化项控制叶子节点数量复杂度(  <span class="math">\(\gamma T\)</span>  )的系数，在文中最后寻找分裂节点  <span class="math">\( j\)</span>  的时候，实际是用分裂后的 <strong>左子树的损失+右子树的损失-分列前的损失</strong></p>
<div class="math">$$
Gain=\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}-\gamma
$$</div>
<p>看  <span class="math">\( Gain\)</span>  是否减少了，也就是判断  <span class="math">\( Gain &gt; 0 \space ?\)</span> ，如果减少了则在  <span class="math">\( j\)</span>  处分裂，反过来看也就是</p>
<div class="math">$$
\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \&gt; \gamma\\
$$</div>
<p><strong><em>gamma</em></strong> (  <span class="math">\( \gamma\)</span>  ) 就是表示的这个数值，意思就是最少要减少  <span class="math">\(\gamma\)</span>  这些损失，否则不分裂</p>
<div class="highlight"><pre><span></span><span class="mi">3</span><span class="p">.</span> <span class="n">max_depth</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">6</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">树的最大深度</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">这个值对结果的影响算是比较大的了，值越大，树的深度越深，模型的复杂度就越高，就越容易过拟合</span>
    <span class="k">c</span><span class="p">:</span> <span class="err">注意如果这个值被设置的较大，会吃掉大量的内存</span>
    <span class="n">d</span><span class="p">:</span> <span class="err">一般来说比价合适的取值区间为</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="mi">4</span><span class="p">.</span> <span class="n">min_child_weight</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">最小的叶子节点权重</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">在普通的</span><span class="n">GBM中</span><span class="err">，叶子节点样本没有权重的概念，其实就是等权重的，也就相当于叶子节点样本个数</span>
    <span class="k">c</span><span class="p">:</span> <span class="err">越小越没有限制，容易过拟合，太高容易欠拟合</span>
</pre></div>
<p><em><strong>min_child_weight</strong></em> 源码中的 <a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L79-L80">文档</a> 是这么解释的，<em>“<strong>Minimum sum of instance weight(hessian) needed in a child.</strong>”</em> 实际上知晓XGBoost的原理的话，其实这个参数就是</p>
<div class="math">$$
H_j = \sum_{i\in I_j}^{}{h^{(i)}}
$$</div>
<div class="math">$$
h^{(i)}= \partial^2_{H_m(x^{(i)})}[l(y^{(i)}, H_m(x^{(i)})]
$$</div>
<p>也就是叶子节点中的样本二阶导求和，更一般的情况假如是均方差损失  <span class="math">\( \frac{1}{2}(y-h_\theta(x))^2\)</span> 的话，二阶导之后的  <span class="math">\( h^{(i)}=1\)</span> ，所以此时 <em><strong>min_child_weight</strong></em> 其实就是叶子节点中的样本个数</p>
<p><em><strong>min_child_weight</strong></em> 数值越大的话，就越不容易形成叶子节点，算法就越保守，越不容易过拟合，其实在XGBoost中，在分裂节点的时候，每个样本是有一个“权重”的概念的，用于在分裂节点的时候减少计算量，权重就是 <span class="math">\( h^{(i)}\)</span></p>
<div class="highlight"><pre><span></span><span class="mi">5</span><span class="p">.</span> <span class="n">max_delta_step</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">inf</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">适用于正负样本不均衡情况下，控制学习速率</span><span class="p">(</span><span class="err">类似</span><span class="n">eta</span><span class="p">)</span><span class="err">最大为某个值，不能超过这个阈值</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">首先我们有参数</span><span class="n">eta来控制学习速率</span><span class="err">，为了后面学习到更多，每一步在权重上乘上这个因子，降低速度</span>
    <span class="k">c</span><span class="p">:</span> <span class="err">但是在正负样本不均衡的情况下</span><span class="n">eta不足够</span><span class="err">，因为此时由于二阶导接近于</span><span class="mi">0</span><span class="err">的原因，权重会特别大</span>
    <span class="n">d</span><span class="p">:</span> <span class="err">这个参数就是用来控制学习速率最大不能超过这个数值</span>

<span class="mi">6</span><span class="p">.</span> <span class="n">sub_sample</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="n">range</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">样本抽样比例</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">在每次训练的随机选取</span><span class="n">sub_sample比例的样本来作为训练样本</span>

<span class="mi">7</span><span class="p">.</span> <span class="n">colsample_by</span><span class="o">*</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">这里实际上有</span><span class="mi">3</span><span class="err">个参数，借助了随机森林的特征抽样的思想，</span><span class="mi">3</span><span class="err">个参数可以同时使用</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">colsample_bytree</span>   <span class="err">更常用，每棵树的特征抽样比例</span>
    <span class="k">c</span><span class="p">:</span> <span class="n">colsample_bylevel</span>  <span class="err">每一层深度的树特征抽样比例</span>
    <span class="n">d</span><span class="p">:</span> <span class="n">colsample_bynode</span>   <span class="err">每一个节点的特征抽样比例</span>

<span class="mi">8</span><span class="p">.</span> <span class="n">lambda</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span> <span class="p">[</span><span class="k">alias</span><span class="p">:</span> <span class="n">reg_lambda</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">损失函数中的</span><span class="n">L2正则化项的系数</span><span class="err">，类似</span><span class="n">RidgeRegression</span><span class="err">，减轻过拟合</span>

<span class="mi">9</span><span class="p">.</span> <span class="n">alpha</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">0</span><span class="p">]</span> <span class="p">[</span><span class="k">alias</span><span class="p">:</span> <span class="n">reg_alpha</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">损失函数中的</span><span class="n">L1正则化项的系数</span><span class="err">，类似</span><span class="n">LASSO</span><span class="err">，减轻过拟合</span>

<span class="mi">10</span><span class="p">.</span> <span class="n">scale_pos_weight</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">在正负样本不均衡的情况下，此参数需要设置，通常为</span><span class="p">:</span> <span class="k">sum</span><span class="p">(</span><span class="err">负样本</span><span class="p">)</span> <span class="o">/</span> <span class="k">sum</span><span class="p">(</span><span class="err">正样本</span><span class="p">)</span>
</pre></div>
<p><strong><em>TreeBooster</em></strong> 的参数整体来说主要为以上这些，当然不止于这些，而是这些是最为常用的，如果想要了解更多的话，<a href="https://xgboost.readthedocs.io/en/latest/parameter.html#">官方文档</a>是个好地方</p>
<h2><strong>学习目标参数 [Learning Task Parameters]</strong></h2>
<div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">.</span> <span class="n">objective</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="n">reg</span><span class="p">:</span><span class="n">squarederror</span><span class="p">(</span><span class="err">均方误差</span><span class="p">)]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">目标函数的选择，默认为均方误差损失，当然还有很多其他的，这里列举几个主要的</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">reg</span><span class="p">:</span><span class="n">squarederror</span>       <span class="err">均方误差</span>
    <span class="k">c</span><span class="p">:</span> <span class="n">reg</span><span class="p">:</span><span class="n">logistic</span>           <span class="err">对数几率损失，参考对数几率回归</span><span class="p">(</span><span class="err">逻辑回归</span><span class="p">)</span>
    <span class="n">d</span><span class="p">:</span> <span class="nb">binary</span><span class="p">:</span><span class="n">logistic</span>        <span class="err">二分类对数几率回归，输出概率值</span>
    <span class="n">e</span><span class="p">:</span> <span class="nb">binary</span><span class="p">:</span><span class="n">hinge</span>           <span class="err">二分类合页损失，此时不输出概率值，而是</span><span class="mi">0</span><span class="err">或</span><span class="mi">1</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">multi</span><span class="p">:</span><span class="n">softmax</span>          <span class="err">多分类</span><span class="n">softmax损失</span><span class="err">，此时需要设置</span><span class="n">num_class参数</span>

<span class="mi">2</span><span class="p">.</span> <span class="n">eval_metric</span> <span class="p">[</span><span class="k">default</span><span class="p">:</span> <span class="err">根据</span><span class="n">objective而定</span><span class="p">]</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">模型性能度量方法，主要根据</span><span class="n">objective而定</span><span class="err">，也可以自定义一些，下面列举一些常见的</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">rmse</span> <span class="p">:</span> <span class="n">root</span> <span class="n">mean</span> <span class="n">square</span> <span class="n">error</span>     <span class="err">也就是平方误差和开根号</span>
    <span class="k">c</span><span class="p">:</span> <span class="n">mae</span>  <span class="p">:</span> <span class="n">mean</span> <span class="k">absolute</span> <span class="n">error</span>        <span class="err">误差的绝对值再求平均</span>
    <span class="n">d</span><span class="p">:</span> <span class="n">auc</span>  <span class="p">:</span> <span class="n">area</span> <span class="k">under</span> <span class="n">curve</span>           <span class="n">roc曲线下面积</span>
    <span class="n">e</span><span class="p">:</span> <span class="n">aucpr</span><span class="p">:</span> <span class="n">area</span> <span class="k">under</span> <span class="n">the</span> <span class="n">pr</span> <span class="n">curve</span>    <span class="n">pr曲线下面积</span>
</pre></div>
<h2><strong>工具包参数 [XGB Packages Parameters]</strong></h2>
<div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">.</span> <span class="n">num_boost_round</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">迭代次数，这货其实跟</span><span class="n">sklearn中的n_estimators是一样的</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">sklearn的api中用n_estimators</span><span class="err">，原始</span><span class="n">xgb中用num_boost_round</span>

<span class="mi">2</span><span class="p">.</span> <span class="n">evals</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">训练过程中通过计算验证集的指标，观察模型性能的数据集</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">指标就是通过</span><span class="n">eval_metric参数来制定的</span>

<span class="mi">3</span><span class="p">.</span> <span class="n">early_stopping_rounds</span>
    <span class="n">a</span><span class="p">:</span> <span class="err">在</span><span class="n">num_boost_round的轮训练中</span><span class="err">，如果过程中指标经过</span><span class="n">early_stopping_rounds轮还没有减少</span>
<span class="err">那么就停止训练</span>
    <span class="n">b</span><span class="p">:</span> <span class="err">指标是通过</span><span class="n">evals的验证集</span><span class="err">，计算</span><span class="n">eval_metric的指标</span>
</pre></div>
<p>这里主要说一下 <em><strong>early_stopping_rounds</strong></em> 这个参数， 首先触发这个参数（也就是确实提前停止了）的时候返回的变量会带有3个属性： <code>best_score</code>, <code>best_iteration</code>, <code>best_ntree_limit</code> ， 这里<code>best_ntree_limit</code> 就是最好的模型的树的个数</p>
<p>但是在文档和 <a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py#L142"> 源码</a> 中，有这么一句话 <strong><em>The method returns the model from the last iteration (not the best one).</em></strong>
就是说如果触发了这个参数的话，那么结果返回的并不是最好的模型，而是最后一轮的模型，那这不坑爹呢？！</p>
<p>但是后续再深入测试的时候发现，用各种指标去验证（比如 <strong><em>rmse</em></strong>
）的时候，结果却和最好的模型是一样的，并不是和最后一轮的模型一样，再深入的研究之后在源码中发现了 <a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py#L142"> 这么一段代码</a> ，XGBoost在调用 <em><strong>predict</strong></em> 的时候 <em><strong>tree_limit</strong></em> 参数如果没指定默认用的就是 <em><strong>best_ntree_limit</strong></em> 也就是在预测时候，用的还是最好的模型</p>
<h2><strong>总结</strong></h2>
<p>整体来说，对于一个机器学习问题如果用XGBoost这个算法包来实现的话，算法层面上的参数主要应用大概是如上这些，当然从0到1实现一个模型的话，这些参数可能是不足够的，还包括工程上的实现，这里只是说在算法方面的一般场景，更明确的其实是 <em><strong>gbtree</strong></em> 这个核心算法，目前来说在实际应用中树模型的表现要好于线性模型，在树模型中提升树表现的还要更好，其实提升树模型中不止 <em><strong>XGBoost</strong></em> 这一种，同样是 <em><strong>GBDT</strong></em> 的升级版还有微软开源的 <em><strong>LightGBM</strong></em> 和 <em><strong>CatBoost</strong></em> ，这个以后有机会再研究。</p>
<hr/>
<p>😛</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div><!-- /.entry-content -->
</article>
</section>
<section class="body" id="extras">
<div class="blogroll">
<h2>about me</h2>
<ul>
<li><a href="https://github.com/reedwave">github</a></li>
<li><a href="https://www.zhihu.com/people/reedqu">zhihu</a></li>
<li><a href="https://zhuanlan.zhihu.com/c_1098916898112212992">知乎专栏</a></li>
</ul>
</div><!-- /.blogroll -->
<div class="social">
<h2>social</h2>
<ul>
<li><a href="https://reed-qu.github.io/feeds/all.atom.xml" rel="alternate" type="application/atom+xml">atom feed</a></li>
</ul>
</div><!-- /.social -->
</section><!-- /#extras -->
<footer class="body" id="contentinfo">
<address class="vcard body" id="about">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
<p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
</footer><!-- /#contentinfo -->
</body>
</html>