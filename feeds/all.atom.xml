<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>[reed@blog ~]# _</title><link href="https://reed-qu.github.io/" rel="alternate"></link><link href="https://reed-qu.github.io/feeds/all.atom.xml" rel="self"></link><id>https://reed-qu.github.io/</id><updated>2020-01-24T22:32:49+08:00</updated><entry><title>跳槽从LeetCode开始</title><link href="https://reed-qu.github.io/tiao-cao-cong-leetcodekai-shi.html" rel="alternate"></link><published>2020-01-24T22:32:49+08:00</published><updated>2020-01-24T22:32:49+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/tiao-cao-cong-leetcodekai-shi.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://leetcode-cn.com"&gt;LeetCode&lt;/a&gt;是一个对很多人来说平常用处不大，但是面试的时候很有用的一个网站，但是之前刷题的时候学英语着实有点痛苦，现在国内也有了对应的官网，轻松多了，前一阵跳槽也是开始了刷题，刷着刷着还觉得挺有意思的，遂起了一个项目，来记录自己的刷题过程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;&lt;a href="https://github.com/reed-qu/leetcode-cn"&gt;GitHub项目&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_preview.png"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;项目的规则&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;用标题的英文作为文件名，去掉空格，驼峰式 …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://leetcode-cn.com"&gt;LeetCode&lt;/a&gt;是一个对很多人来说平常用处不大，但是面试的时候很有用的一个网站，但是之前刷题的时候学英语着实有点痛苦，现在国内也有了对应的官网，轻松多了，前一阵跳槽也是开始了刷题，刷着刷着还觉得挺有意思的，遂起了一个项目，来记录自己的刷题过程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;&lt;a href="https://github.com/reed-qu/leetcode-cn"&gt;GitHub项目&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_preview.png"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;项目的规则&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;用标题的英文作为文件名，去掉空格，驼峰式 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;commit -m "message"&lt;/code&gt; 中 &lt;code&gt;message&lt;/code&gt; 的格式为： &lt;code&gt;题目 | issue id | 内容&lt;/code&gt; ，如: &lt;code&gt;242. 有效的字母异位词 | #64 | 添加新的解法&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;issue&lt;/code&gt; 是原始问题的还原，1个问题对应1个 &lt;code&gt;issue&lt;/code&gt; ，同时给 &lt;code&gt;issue&lt;/code&gt; 打上 &lt;code&gt;label&lt;/code&gt; （题目难度、题目类型等），把code文件链接回复到对应的issue中 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_commit.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_issue_list.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_issue_detail.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Code文件&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;QUESTION&lt;/code&gt; 原始问题的题设 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;THINKING&lt;/code&gt; 解题思路 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Solution&lt;/code&gt; 类，即官方给的类和方法的定义 &lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;if __name__ == "__main__"&lt;/code&gt; 中提供1个测试用例，能让 &lt;code&gt;py&lt;/code&gt; 脚本直接跑起来 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/leetcode_project_code.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;这个项目应该会长时间做下去，算不上是为开源做贡献，起码也是个记事本，跳槽的时候还能用上，而且有些题确实是有点意思的，如果这个项目有幸帮助到你，不考虑 &lt;code&gt;star&lt;/code&gt; 一下？&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😝&lt;/p&gt;</content><category term="machine learning"></category></entry><entry><title>XGBoost调参详解</title><link href="https://reed-qu.github.io/xgboostdiao-can-xiang-jie.html" rel="alternate"></link><published>2020-01-24T22:20:14+08:00</published><updated>2020-01-24T22:20:14+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/xgboostdiao-can-xiang-jie.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在之前的一篇文章中，从 &lt;strong&gt;GBDT&lt;/strong&gt; 一直说到当下最流行的梯度提升树模型之一 &lt;strong&gt;&lt;a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"&gt;&lt;em&gt;XGBoost&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;，今天这里主要说应用XGB这个算法包的一些参数问题，在实际应用中，我们并不会自己动手去实现一个XGB，了解更多的XGB的算法原理，也是为了我们在工程实践中更好的应对各种问题，了解每一个参数背后的意义，有助我们训练出来更好的模型&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;普通参数 [General Parameters …&lt;/strong&gt;&lt;/h2&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在之前的一篇文章中，从 &lt;strong&gt;GBDT&lt;/strong&gt; 一直说到当下最流行的梯度提升树模型之一 &lt;strong&gt;&lt;a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"&gt;&lt;em&gt;XGBoost&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;，今天这里主要说应用XGB这个算法包的一些参数问题，在实际应用中，我们并不会自己动手去实现一个XGB，了解更多的XGB的算法原理，也是为了我们在工程实践中更好的应对各种问题，了解每一个参数背后的意义，有助我们训练出来更好的模型&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;普通参数 [General Parameters]&lt;/strong&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;booster&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;gbtree&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;表示应用的弱学习器的类型&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;推荐用默认参数&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;可选的有&lt;/span&gt;&lt;span class="n"&gt;gbtree&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dart&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gblinear&lt;/span&gt;
        &lt;span class="n"&gt;gblinear是线性模型&lt;/span&gt;&lt;span class="err"&gt;，表现很差，接近一个&lt;/span&gt;&lt;span class="n"&gt;LASSO&lt;/span&gt;
        &lt;span class="n"&gt;dart是树模型的一种&lt;/span&gt;&lt;span class="err"&gt;，思想是每次训练新树的时候，随机从前&lt;/span&gt;&lt;span class="n"&gt;m轮的树中扔掉一些&lt;/span&gt;&lt;span class="err"&gt;，来避免过拟合&lt;/span&gt;
        &lt;span class="n"&gt;gbtree即是论文中主要讨论的树模型&lt;/span&gt;&lt;span class="err"&gt;，推荐使用&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;silent&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;不推荐&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;不推荐使用，推荐使用&lt;/span&gt;&lt;span class="n"&gt;verbosity参数来代替&lt;/span&gt;&lt;span class="err"&gt;，功能更强大&lt;/span&gt;

&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;verbosity&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;训练过程中打印的日志等级，&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;silent&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;nthread&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;最大可用线程数&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="k"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;训练过程中的并行线程数&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;如果用的是&lt;/span&gt;&lt;span class="n"&gt;sklearn的api&lt;/span&gt;&lt;span class="err"&gt;，那么使用&lt;/span&gt;&lt;span class="n"&gt;n_jobs来代替&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;每个分类器算法参数 [Booster Parameters]&lt;/strong&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;就是常说的学习速率，控制每一次学习的权重缩减，给后来的模型提供更多的学习空间&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;min_split_loss&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;叶子节点分裂时所需要的最小的损失减少量，这个值越大，叶子节点越难分裂，所以算法就越保守&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;gamma&lt;/em&gt;&lt;/strong&gt; (  &lt;span class="math"&gt;\( \gamma\)&lt;/span&gt;  )就是 &lt;a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"&gt; 从GBDT到XGBoost&lt;/a&gt; 中的正则化项控制叶子节点数量复杂度(  &lt;span class="math"&gt;\(\gamma T\)&lt;/span&gt;  )的系数，在文中最后寻找分裂节点  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  的时候，实际是用分裂后的 &lt;strong&gt;左子树的损失+右子树的损失-分列前的损失&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
Gain=\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}-\gamma
$$&lt;/div&gt;
&lt;p&gt;看  &lt;span class="math"&gt;\( Gain\)&lt;/span&gt;  是否减少了，也就是判断  &lt;span class="math"&gt;\( Gain &amp;gt; 0 \space ?\)&lt;/span&gt; ，如果减少了则在  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  处分裂，反过来看也就是&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \&amp;gt; \gamma\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;gamma&lt;/em&gt;&lt;/strong&gt; (  &lt;span class="math"&gt;\( \gamma\)&lt;/span&gt;  ) 就是表示的这个数值，意思就是最少要减少  &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;  这些损失，否则不分裂&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;树的最大深度&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;这个值对结果的影响算是比较大的了，值越大，树的深度越深，模型的复杂度就越高，就越容易过拟合&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;注意如果这个值被设置的较大，会吃掉大量的内存&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;一般来说比价合适的取值区间为&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;min_child_weight&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;最小的叶子节点权重&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;在普通的&lt;/span&gt;&lt;span class="n"&gt;GBM中&lt;/span&gt;&lt;span class="err"&gt;，叶子节点样本没有权重的概念，其实就是等权重的，也就相当于叶子节点样本个数&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;越小越没有限制，容易过拟合，太高容易欠拟合&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;&lt;strong&gt;min_child_weight&lt;/strong&gt;&lt;/em&gt; 源码中的 &lt;a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L79-L80"&gt;文档&lt;/a&gt; 是这么解释的，&lt;em&gt;“&lt;strong&gt;Minimum sum of instance weight(hessian) needed in a child.&lt;/strong&gt;”&lt;/em&gt; 实际上知晓XGBoost的原理的话，其实这个参数就是&lt;/p&gt;
&lt;div class="math"&gt;$$
H_j = \sum_{i\in I_j}^{}{h^{(i)}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
h^{(i)}= \partial^2_{H_m(x^{(i)})}[l(y^{(i)}, H_m(x^{(i)})]
$$&lt;/div&gt;
&lt;p&gt;也就是叶子节点中的样本二阶导求和，更一般的情况假如是均方差损失  &lt;span class="math"&gt;\( \frac{1}{2}(y-h_\theta(x))^2\)&lt;/span&gt; 的话，二阶导之后的  &lt;span class="math"&gt;\( h^{(i)}=1\)&lt;/span&gt; ，所以此时 &lt;em&gt;&lt;strong&gt;min_child_weight&lt;/strong&gt;&lt;/em&gt; 其实就是叶子节点中的样本个数&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;min_child_weight&lt;/strong&gt;&lt;/em&gt; 数值越大的话，就越不容易形成叶子节点，算法就越保守，越不容易过拟合，其实在XGBoost中，在分裂节点的时候，每个样本是有一个“权重”的概念的，用于在分裂节点的时候减少计算量，权重就是 &lt;span class="math"&gt;\( h^{(i)}\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;max_delta_step&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;适用于正负样本不均衡情况下，控制学习速率&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;类似&lt;/span&gt;&lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;最大为某个值，不能超过这个阈值&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;首先我们有参数&lt;/span&gt;&lt;span class="n"&gt;eta来控制学习速率&lt;/span&gt;&lt;span class="err"&gt;，为了后面学习到更多，每一步在权重上乘上这个因子，降低速度&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;但是在正负样本不均衡的情况下&lt;/span&gt;&lt;span class="n"&gt;eta不足够&lt;/span&gt;&lt;span class="err"&gt;，因为此时由于二阶导接近于&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="err"&gt;的原因，权重会特别大&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;这个参数就是用来控制学习速率最大不能超过这个数值&lt;/span&gt;

&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;sub_sample&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;样本抽样比例&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;在每次训练的随机选取&lt;/span&gt;&lt;span class="n"&gt;sub_sample比例的样本来作为训练样本&lt;/span&gt;

&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;colsample_by&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;这里实际上有&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="err"&gt;个参数，借助了随机森林的特征抽样的思想，&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="err"&gt;个参数可以同时使用&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;colsample_bytree&lt;/span&gt;   &lt;span class="err"&gt;更常用，每棵树的特征抽样比例&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;colsample_bylevel&lt;/span&gt;  &lt;span class="err"&gt;每一层深度的树特征抽样比例&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;colsample_bynode&lt;/span&gt;   &lt;span class="err"&gt;每一个节点的特征抽样比例&lt;/span&gt;

&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reg_lambda&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;损失函数中的&lt;/span&gt;&lt;span class="n"&gt;L2正则化项的系数&lt;/span&gt;&lt;span class="err"&gt;，类似&lt;/span&gt;&lt;span class="n"&gt;RidgeRegression&lt;/span&gt;&lt;span class="err"&gt;，减轻过拟合&lt;/span&gt;

&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reg_alpha&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;损失函数中的&lt;/span&gt;&lt;span class="n"&gt;L1正则化项的系数&lt;/span&gt;&lt;span class="err"&gt;，类似&lt;/span&gt;&lt;span class="n"&gt;LASSO&lt;/span&gt;&lt;span class="err"&gt;，减轻过拟合&lt;/span&gt;

&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;scale_pos_weight&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;在正负样本不均衡的情况下，此参数需要设置，通常为&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;负样本&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="k"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;正样本&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;TreeBooster&lt;/em&gt;&lt;/strong&gt; 的参数整体来说主要为以上这些，当然不止于这些，而是这些是最为常用的，如果想要了解更多的话，&lt;a href="https://xgboost.readthedocs.io/en/latest/parameter.html#"&gt;官方文档&lt;/a&gt;是个好地方&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;学习目标参数 [Learning Task Parameters]&lt;/strong&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;objective&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;squarederror&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;均方误差&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;目标函数的选择，默认为均方误差损失，当然还有很多其他的，这里列举几个主要的&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;squarederror&lt;/span&gt;       &lt;span class="err"&gt;均方误差&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;logistic&lt;/span&gt;           &lt;span class="err"&gt;对数几率损失，参考对数几率回归&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;逻辑回归&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;logistic&lt;/span&gt;        &lt;span class="err"&gt;二分类对数几率回归，输出概率值&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;hinge&lt;/span&gt;           &lt;span class="err"&gt;二分类合页损失，此时不输出概率值，而是&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="err"&gt;或&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;multi&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;          &lt;span class="err"&gt;多分类&lt;/span&gt;&lt;span class="n"&gt;softmax损失&lt;/span&gt;&lt;span class="err"&gt;，此时需要设置&lt;/span&gt;&lt;span class="n"&gt;num_class参数&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;eval_metric&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;根据&lt;/span&gt;&lt;span class="n"&gt;objective而定&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;模型性能度量方法，主要根据&lt;/span&gt;&lt;span class="n"&gt;objective而定&lt;/span&gt;&lt;span class="err"&gt;，也可以自定义一些，下面列举一些常见的&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;rmse&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;     &lt;span class="err"&gt;也就是平方误差和开根号&lt;/span&gt;
    &lt;span class="k"&gt;c&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mae&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="k"&gt;absolute&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;        &lt;span class="err"&gt;误差的绝对值再求平均&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;auc&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;area&lt;/span&gt; &lt;span class="k"&gt;under&lt;/span&gt; &lt;span class="n"&gt;curve&lt;/span&gt;           &lt;span class="n"&gt;roc曲线下面积&lt;/span&gt;
    &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;aucpr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;area&lt;/span&gt; &lt;span class="k"&gt;under&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;pr&lt;/span&gt; &lt;span class="n"&gt;curve&lt;/span&gt;    &lt;span class="n"&gt;pr曲线下面积&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;&lt;strong&gt;工具包参数 [XGB Packages Parameters]&lt;/strong&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;num_boost_round&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;迭代次数，这货其实跟&lt;/span&gt;&lt;span class="n"&gt;sklearn中的n_estimators是一样的&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sklearn的api中用n_estimators&lt;/span&gt;&lt;span class="err"&gt;，原始&lt;/span&gt;&lt;span class="n"&gt;xgb中用num_boost_round&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;训练过程中通过计算验证集的指标，观察模型性能的数据集&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;指标就是通过&lt;/span&gt;&lt;span class="n"&gt;eval_metric参数来制定的&lt;/span&gt;

&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;early_stopping_rounds&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;在&lt;/span&gt;&lt;span class="n"&gt;num_boost_round的轮训练中&lt;/span&gt;&lt;span class="err"&gt;，如果过程中指标经过&lt;/span&gt;&lt;span class="n"&gt;early_stopping_rounds轮还没有减少&lt;/span&gt;
&lt;span class="err"&gt;那么就停止训练&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;指标是通过&lt;/span&gt;&lt;span class="n"&gt;evals的验证集&lt;/span&gt;&lt;span class="err"&gt;，计算&lt;/span&gt;&lt;span class="n"&gt;eval_metric的指标&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里主要说一下 &lt;em&gt;&lt;strong&gt;early_stopping_rounds&lt;/strong&gt;&lt;/em&gt; 这个参数， 首先触发这个参数（也就是确实提前停止了）的时候返回的变量会带有3个属性： &lt;code&gt;best_score&lt;/code&gt;, &lt;code&gt;best_iteration&lt;/code&gt;, &lt;code&gt;best_ntree_limit&lt;/code&gt; ， 这里&lt;code&gt;best_ntree_limit&lt;/code&gt; 就是最好的模型的树的个数&lt;/p&gt;
&lt;p&gt;但是在文档和 &lt;a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py#L142"&gt; 源码&lt;/a&gt; 中，有这么一句话 &lt;strong&gt;&lt;em&gt;The method returns the model from the last iteration (not the best one).&lt;/em&gt;&lt;/strong&gt;
就是说如果触发了这个参数的话，那么结果返回的并不是最好的模型，而是最后一轮的模型，那这不坑爹呢？！&lt;/p&gt;
&lt;p&gt;但是后续再深入测试的时候发现，用各种指标去验证（比如 &lt;strong&gt;&lt;em&gt;rmse&lt;/em&gt;&lt;/strong&gt;
）的时候，结果却和最好的模型是一样的，并不是和最后一轮的模型一样，再深入的研究之后在源码中发现了 &lt;a href="https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/training.py#L142"&gt; 这么一段代码&lt;/a&gt; ，XGBoost在调用 &lt;em&gt;&lt;strong&gt;predict&lt;/strong&gt;&lt;/em&gt; 的时候 &lt;em&gt;&lt;strong&gt;tree_limit&lt;/strong&gt;&lt;/em&gt; 参数如果没指定默认用的就是 &lt;em&gt;&lt;strong&gt;best_ntree_limit&lt;/strong&gt;&lt;/em&gt; 也就是在预测时候，用的还是最好的模型&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;整体来说，对于一个机器学习问题如果用XGBoost这个算法包来实现的话，算法层面上的参数主要应用大概是如上这些，当然从0到1实现一个模型的话，这些参数可能是不足够的，还包括工程上的实现，这里只是说在算法方面的一般场景，更明确的其实是 &lt;em&gt;&lt;strong&gt;gbtree&lt;/strong&gt;&lt;/em&gt; 这个核心算法，目前来说在实际应用中树模型的表现要好于线性模型，在树模型中提升树表现的还要更好，其实提升树模型中不止 &lt;em&gt;&lt;strong&gt;XGBoost&lt;/strong&gt;&lt;/em&gt; 这一种，同样是 &lt;em&gt;&lt;strong&gt;GBDT&lt;/strong&gt;&lt;/em&gt; 的升级版还有微软开源的 &lt;em&gt;&lt;strong&gt;LightGBM&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;CatBoost&lt;/strong&gt;&lt;/em&gt; ，这个以后有机会再研究。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>SVM(三)从软间隔到核函数</title><link href="https://reed-qu.github.io/svmsan-cong-ruan-jian-ge-dao-he-han-shu.html" rel="alternate"></link><published>2020-01-24T22:15:11+08:00</published><updated>2020-01-24T22:15:11+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/svmsan-cong-ruan-jian-ge-dao-he-han-shu.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html"&gt; SVM(一) &lt;/a&gt; 和 &lt;a href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html"&gt; SVM(二)&lt;/a&gt; 中，从 &lt;strong&gt;感知机&lt;/strong&gt; 到 &lt;strong&gt;SVM损失函数&lt;/strong&gt; 再到&lt;strong&gt;拉格朗日对偶式，&lt;/strong&gt; 个人认为还是较为清楚的说了 &lt;strong&gt;硬间隔线性可分SVM …&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html"&gt; SVM(一) &lt;/a&gt; 和 &lt;a href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html"&gt; SVM(二)&lt;/a&gt; 中，从 &lt;strong&gt;感知机&lt;/strong&gt; 到 &lt;strong&gt;SVM损失函数&lt;/strong&gt; 再到&lt;strong&gt;拉格朗日对偶式，&lt;/strong&gt; 个人认为还是较为清楚的说了 &lt;strong&gt;硬间隔线性可分SVM&lt;/strong&gt; ，但是现实世界中往往并不是完美的，在实际环境中接触到的数据即便是线性可分也往往是找不到硬间隔的，还有线性不可分的情况，那么此时应该怎么办，SVM如何解决这种问题，本次SVM的最后一部分就来说一说这些问题&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;软间隔线性可分SVM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;首先依旧是给定数据集  &lt;span class="math"&gt;\(D={(x^{(1)}, y^{(1)}), (x^{(2)},y^{(2)}),\cdots,(x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\(x^{(i)}\in R^n\)&lt;/span&gt;，&lt;span class="math"&gt;\(y^{(i)}\in {-1, +1}\)&lt;/span&gt;，&lt;span class="math"&gt;\(i = 1,2...,N\)&lt;/span&gt;  ，前面的问题中&lt;strong&gt;前提是数据一定得是线性可分&lt;/strong&gt; 的，我们这里也假定数据是线性可分的，但是硬间隔不存在，也就是说 &lt;strong&gt;一些噪声或者叫异常点在间隔中干扰了硬间隔&lt;/strong&gt;，导致找不到一个传统意义上的硬间隔。&lt;/p&gt;
&lt;p&gt;此时既然仍然是线性可分的，那就还是可以在数据中画出一个超平面尽最大限度的分好正负例，且能找到最大间隔，只不过间隔是有“误差”的，换句话就是说，此时允许一定的“误差”落在间隔中，这种间隔就叫做&lt;strong&gt;软间隔&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm3_soft_margin.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;从图中可以清楚的看出来，相较于硬间隔，软间隔允许有 &lt;strong&gt;错分数据&lt;/strong&gt; 在间隔内，此时在数学上达到这种效果的方法就是给每一个样本点一个 &lt;strong&gt;松弛变量&lt;/strong&gt; &lt;span class="math"&gt;\(\xi\)&lt;/span&gt;  ，且和最大间隔  &lt;span class="math"&gt;\(\frac{1}{2}||\theta||^2\)&lt;/span&gt;  之间加一个 &lt;strong&gt;惩罚参数来控制二者的“权重”&lt;/strong&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt; ，比如  &lt;span class="math"&gt;\(C=+\infty\)&lt;/span&gt; 时其实就相当于不允许有任何误差，也就容易过拟合，所以软、硬间隔线性可分SVM二者的损失函数是类似的，主要差别即在 &lt;strong&gt;松弛变量&lt;/strong&gt; 这部分，定义软间隔线性可分SVM损失函数为&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \mathop{min}_{\theta}\space&amp;amp;\frac{1}{2}||\theta||^2+C\sum_{i=1}^{N}{\xi^{(i)}}\\
        s.t. \space&amp;amp;y^{(i)}(\theta^Tx^{(i)}+b)+\xi^{(i)}\geq1，&amp;amp;i=1,2,...,N\\
                   &amp;amp;\xi^{(i)}\geq0，                           &amp;amp;i=1,2,...,N
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;然后依然是构造拉格朗日函数，但是注意此时约束为 &lt;strong&gt;两个不等式&lt;/strong&gt; ，所以需要  &lt;span class="math"&gt;\(\alpha,\beta\)&lt;/span&gt; 两个拉格朗日乘子向量&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \mathop{min}_{\theta,b,\xi}\mathop{max}_{\alpha,\beta}L(\theta,b,\alpha,\beta,\xi)=
        &amp;amp;\frac{1}{2}||\theta||^2+C\sum_{i=1}^{N}{\xi^{(i)}}\\
        &amp;amp;+\sum_{i=1}^{N}{\alpha^{i}}(1-y^{(i)}(\theta^Tx^{(i)}+b)-\xi^{(i)})\\
        &amp;amp;+\sum_{i=1}^{N}{\beta^{(i)}\xi^{(i)}}\\
        =&amp;amp;\mathop{max}_{\alpha,\beta}\mathop{min}_{\theta,b,\xi}L(\theta,b,\alpha,\beta,\xi)
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;然后根据对偶问题进行计算，分别对  &lt;span class="math"&gt;\(\theta,b,\xi\)&lt;/span&gt;  求导等于  &lt;span class="math"&gt;\(0\)&lt;/span&gt;  求极小&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial \theta}&amp;amp;= \theta-\sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}x^{(i)}}&amp;amp;=0\\
        \frac{\partial L}{\partial b} &amp;amp;=-\sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}}&amp;amp;=0\\
        \frac{\partial L}{\partial \xi^{(i)}}&amp;amp;=C-\alpha^{(i)}-\beta^{(i)}&amp;amp;=0
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;整合公式即可得出， &lt;strong&gt;不要忘记拉格朗日乘子的约束&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp;\theta=\sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}x^{(i)}}\\
    &amp;amp;\sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}}=0\\
    &amp;amp;C-\alpha^{(i)}-\beta^{(i)}=0\\
    &amp;amp;\alpha^{(i)}\geq0\\
    &amp;amp;\beta^{(i)}\geq0
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;然后将  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  带入原始问题的拉格朗日函数中，此处与硬间隔一致，所以推导过程就不赘述了，可以参考 &lt;a href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html"&gt;SVM(二)拉格朗日对偶式 &lt;/a&gt; ，得到&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{min}_{\theta,b,\xi}L(\theta,b,\alpha,\beta,\xi)=\sum_{i=1}^{N}{\alpha^{(i)}}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha^{(i)}\alpha^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}\\
$$&lt;/div&gt;
&lt;p&gt;再对  &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;  求  &lt;span class="math"&gt;\(L\)&lt;/span&gt;的极大即是对偶问题了，这里先考虑相较于硬间隔SVM来说多出来的约束条件  &lt;span class="math"&gt;\(C-\alpha^{(i)}-\beta^{(i)}=0\)&lt;/span&gt;  ，和  &lt;span class="math"&gt;\(\alpha^{(i)}\geq0，\beta^{(i)}\geq0\)&lt;/span&gt;  ，可以约掉  &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;  ，即是&lt;span class="math"&gt;\(0\leq\alpha^{(i)}\leq C\)&lt;/span&gt; ，所以对偶问题描述成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \mathop{min}_{\alpha}\space
        &amp;amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha^{(i)}\alpha^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}-\sum_{i=1}^{N}{\alpha^{(i)}}\\
        s.t. \space&amp;amp; \sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}=0}\\
        &amp;amp;0\leq\alpha^{(i)}\leq C，i=1,2,...,N
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;从对偶问题的形式也可以看出来，相较于硬间隔SVM来说，只是对拉格朗日乘子的上限做了限制，上限就是&lt;/strong&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;核方法&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;软间隔线性可分依然是线性可分，假如真实的数据是线性不可分的情况SVM能不能处理，怎么处理？比如说数据分布如下图(左)，这时候是数据线性不可分，但是在可视化图中是能够很清楚的知道这些数据应该怎么划分，这里超平面应该是一个圆，但是我们不能完全依赖数据可视化，超过3维肉眼就很难看出来了，&lt;strong&gt;这时候的一个思路是经过一个特殊的转化(下图&lt;/strong&gt; &lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt; &lt;strong&gt;)，把这些数据变成线性可分的(下图右)，那么就可以利用线性可分SVM来处理了&lt;/strong&gt;，核方法就是这么处理的。其实核方法不是只在SVM中出现，核方法是一个纯粹的数学方法，其解决的问题就是 &lt;strong&gt;数据映射到高维空间中计算量过于复杂&lt;/strong&gt; 的问题&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm3_kernel.png"/&gt;&lt;/p&gt;
&lt;p&gt;根据线性可分SVM中推导的公式来看，最终的目标函数只是与  &lt;span class="math"&gt;\(x^{(i)},x^{(j)}\)&lt;/span&gt;  的 &lt;strong&gt;内积&lt;/strong&gt; 有关，然后对于线性不可分的数据定义某种线性变化&lt;/p&gt;
&lt;div class="math"&gt;$$
x\rightarrow \phi(x)\\
$$&lt;/div&gt;
&lt;p&gt;把 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 映射到高维空间中，目的是 &lt;strong&gt;为了让在低维空间中线性不可分的数据在映射后的高位空间中线性可分&lt;/strong&gt;，映射后的内积就是  &lt;span class="math"&gt;\(&amp;lt;\phi(x^{(i)}),\phi(x^{(j)})&amp;gt;\)&lt;/span&gt;
，有了这个映射就可以在映射后的高维空间中做SVM的事情，但是这个不叫核方法。到这里其实仔细想一下就会发现一个问题，这个映射是要做“&lt;strong&gt;升维&lt;/strong&gt;”的事儿，那么维度爆炸甚至是无穷维时怎么办，计算量会特别大，解决这个问题的方法才是核方法，具体的做法是在原始空间中找到一个函数 &lt;span class="math"&gt;\(\kappa\)&lt;/span&gt;  ，使得&lt;/p&gt;
&lt;div class="math"&gt;$$
\kappa(x^{(i)},x^{(j)})=&amp;lt;\phi(x^{(i)}),\phi(x^{(j)})&amp;gt;\\
$$&lt;/div&gt;
&lt;p&gt;这样一来根本就不用去计算  &lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;  了甚至不需要知道  &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; 是怎么映射的就解决了维度爆炸带来的计算性能下降的问题，找到的这个函数  &lt;span class="math"&gt;\(\kappa\)&lt;/span&gt; 就叫做核函数。常用的核函数主要为以下3类&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;线性核，实际上就是没有做变换&lt;/strong&gt; &lt;span class="math"&gt;\(\kappa(x_1, x_2) = &amp;lt;x_1, x_2&amp;gt;\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高斯核，使用较为广泛，能够把原始特征映射到无穷维，也叫做径向基函数(Radial Basic Function，简称RBF)&lt;/strong&gt; &lt;span class="math"&gt;\(\kappa(x_1, x_2) = exp(-\frac{||x_1-x_2||_2^2}{2\sigma^2})\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多项式核&lt;/strong&gt; &lt;span class="math"&gt;\(\kappa(x_1, x_2) = (&amp;lt;x_1, x_2&amp;gt;+R)^d\)&lt;/span&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里拿多项式核举个例子，取  &lt;span class="math"&gt;\(R=0，d=2\)&lt;/span&gt;  ，假设这里的  &lt;span class="math"&gt;\(x，y\)&lt;/span&gt;  是一个2维向量&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \kappa(x,y) &amp;amp;= (&amp;lt;x, y&amp;gt;)^2\\
        &amp;amp;=(x_1y_1+x_2y_2)^2\\\ &amp;amp;=x_1^2y_1^2+2x_1y_1x_2y_2+x_2^2y_2^2\\
        &amp;amp;=&amp;lt;(x_1^2,\sqrt2x_1x_2,x_2^2),(y_1^2,\sqrt2y_1y_2,y_2^2) &amp;gt;\\
        &amp;amp;=&amp;lt;\phi(x),\phi(y)&amp;gt;
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;可以看出来，核函数  &lt;span class="math"&gt;\(\kappa\)&lt;/span&gt; 可以避免在高维空间中做内积这种复杂的运算是因为核函数巧妙的避免了映射之后再做内积，而是直接给替代了，想对应的SVM的原始问题、对偶问题、超平面模型都会有所变化，即是将内积转变为其核函数&lt;/p&gt;
&lt;div class="math"&gt;$$
x^{(i)}\cdot x^{(j)}\rightarrow \kappa(x^{(i)}, x^{(j)})\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;软间隔对偶问题&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \mathop{min}_{\alpha}\space
        &amp;amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\alpha^{(i)}\alpha^{(j)}y^{(i)}y^{(j)}\kappa(x^{(i)},x^{(j)})}-\sum_{i=1}^{N}{\alpha^{(i)}}\\
        s.t. \space&amp;amp; \sum_{i=1}^{N}{\alpha^{(i)}y^{(i)}=0}\\
        &amp;amp;0\leq\alpha^{(i)}\leq C，i=1,2,...,N
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;超平面模型&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x)=sign(\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}\kappa(x\cdot x^{(i)})+b^*})\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;经过 &lt;strong&gt;&lt;a href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html"&gt; SVM(一)从感知机到SVM损失函数 &lt;/a&gt; &lt;/strong&gt; 和 &lt;strong&gt;&lt;a href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html"&gt;SVM(二)拉格朗日对偶式 &lt;/a&gt; &lt;/strong&gt;和本文，对SVM做了简单的介绍，其实还有一些内容没有整理，比如求解对偶问题的 &lt;strong&gt;SMO算法(类似坐标轴下降的一种算法)&lt;/strong&gt; ，还有 &lt;strong&gt;支持向量回归&lt;/strong&gt;等等，这部分留给后面处理。当然SVM并不可能是万能的，也是有一定使用场景的&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;较适合解决高维特征的分类回归问题，因为SVM主要求解的是对偶问题，经过对偶问题之后高维度就转化成了样本量相关的问题&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;通过核方法来能够解决线性不可分问题&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;还是因为对偶问题会把高维度转化成样本量相关的问题的原因，SVM并不适用于特别大的数据集&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;核函数的选择更像是玄学，并没有完备的理论依据来证明什么核函数适用于什么情况，所以往往需要尝试，利用交叉验证来选取更合适的核函数&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;较依赖好的特征工程，特别是缺失值&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;最后感谢李航老师的&lt;a href="https://book.douban.com/subject/33437381/"&gt;《统计学习方法 第二版》&lt;/a&gt; ，对我个人理解SVM有很大帮助。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>SVM(二)拉格朗日对偶式</title><link href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html" rel="alternate"></link><published>2020-01-24T21:11:26+08:00</published><updated>2020-01-24T21:11:26+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/svmer-la-ge-lang-ri-dui-ou-shi.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html"&gt; SVM(一) &lt;/a&gt;中从感知机说到硬间隔线性可分SVM的损失函数，此时损失函数是有约束条件的，无法直接计算导数为0时的参数，需要转换一下&lt;strong&gt;将目标函数和约束条件放到一个拉格朗日函数中&lt;/strong&gt; ，这也就是为什么要构造拉格朗日函数，同时引出 &lt;strong&gt;拉格朗日对偶式&lt;/strong&gt;，利用对偶式来求解，并且有利于引出核函数的思想，这里简单讨论一下SVM对偶式，并给出一个具体例子来思考原始问题和对偶问题的异同点 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html"&gt; SVM(一) &lt;/a&gt;中从感知机说到硬间隔线性可分SVM的损失函数，此时损失函数是有约束条件的，无法直接计算导数为0时的参数，需要转换一下&lt;strong&gt;将目标函数和约束条件放到一个拉格朗日函数中&lt;/strong&gt; ，这也就是为什么要构造拉格朗日函数，同时引出 &lt;strong&gt;拉格朗日对偶式&lt;/strong&gt;，利用对偶式来求解，并且有利于引出核函数的思想，这里简单讨论一下SVM对偶式，并给出一个具体例子来思考原始问题和对偶问题的异同点。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;拉格朗日乘子&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;首先给定线性可分数据集  &lt;span class="math"&gt;\( D={(x^{(1)}, y^{(1)}), (x^{(2)},y^{(2)}),\cdots,(x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\( x^{(i)}\in R^n\)&lt;/span&gt; ， &lt;span class="math"&gt;\( y^{(i)}\in {-1, +1}\)&lt;/span&gt;  &lt;span class="math"&gt;\( i = 1,2...,N\)&lt;/span&gt;，观察SVM的损失函数，这是一个有约束条件的损失函数，也就是说要在 &lt;strong&gt;约束条件中找到目标函数的最小值&lt;/strong&gt; ，即  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  在满足  &lt;span class="math"&gt;\( s.t. \space y^{(i)}(\theta^Tx^{(i)}+b)\geq1\)&lt;/span&gt; 的基础上，最小化  &lt;span class="math"&gt;\( \frac{1}{2}||\theta||^2\)&lt;/span&gt;  ，而且是 &lt;strong&gt;不等式优化&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp;\mathop{min}_{\theta}\frac{1}{2}||\theta||^2\\
    &amp;amp;s.t. \space y^{(i)}(\theta^Tx^{(i)}+b)\geq1
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;针对此约束条件，考虑一般形式  &lt;span class="math"&gt;\( f(x); \space\space s.t. \space g(x)\leq 0\)&lt;/span&gt; ，有2种情况(图中绿色和蓝色的两种条件)&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm2_lagrange.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一种&lt;/strong&gt; ：图中蓝色  &lt;span class="math"&gt;\( g(x)\leq 0\)&lt;/span&gt;  部分，目标函数的 &lt;strong&gt;最优解在约束条件范围外&lt;/strong&gt;，此时就相当于约束条件为  &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt;  ，因为  &lt;span class="math"&gt;\( g(x)&amp;lt;0\)&lt;/span&gt;  的部分相较于 &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt;  肯定不是最优的(不够靠近目标函数的最优解嘛)，这里通过图中可以直观的看出， &lt;strong&gt;切线交点&lt;/strong&gt; &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  &lt;strong&gt;为最优点 ，是符合约束条件时的最小值，此时二者导数的方向相反&lt;/strong&gt;，这里与2个注意点需要解释&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;为什么 &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  是切线交点？ &lt;/strong&gt; 可以利用反证法的思想，假如  &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  不是切线交点，那么 &lt;strong&gt;一定有不为 &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  的分量使得目标函数更小一些 &lt;/strong&gt; ，那么显然那个点不是最优点，一直移动到  &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  点时为最优 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么导数的方向相反？&lt;/strong&gt; 首先  &lt;span class="math"&gt;\( \bigtriangledown g(x)\)&lt;/span&gt;  是垂直  &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt;  的，而最小化的目标函数  &lt;span class="math"&gt;\( \bigtriangledown f(x)\)&lt;/span&gt;  在切线交点  &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  位置也是垂直  &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt;  的， &lt;strong&gt;且相反的时候为最小值&lt;/strong&gt; ， &lt;strong&gt;相同的时候为最大值&lt;/strong&gt; ，这样一来就可以写成  &lt;span class="math"&gt;\( \bigtriangledown f(x) + \lambda\bigtriangledown g(x) = 0\)&lt;/span&gt;  ，这里  &lt;span class="math"&gt;\( \lambda &amp;gt; 0\)&lt;/span&gt;  为一个系数， &lt;strong&gt;二者向量总是可以通过 &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  这个系数转化为大小相等的 &lt;/strong&gt; ，那么这样就可以构造出 &lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
L(x, \lambda) = f(x) + \lambda g(x)\\
$$&lt;/div&gt;
&lt;p&gt;为什么要这么构造也很容易理解，对 &lt;span class="math"&gt;\( L(x, \lambda)\)&lt;/span&gt;  这个公式网回算，分别对  &lt;span class="math"&gt;\( x, \lambda\)&lt;/span&gt;  求导等于 &lt;span class="math"&gt;\( 0\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \frac{\partial L(x, \lambda)}{\partial x} &amp;amp;= \bigtriangledown f(x) +\lambda\bigtriangledown g(x)&amp;amp;= 0\\
    \frac{\partial L(x, \lambda)}{\partial \lambda} &amp;amp;= g(x)&amp;amp;= 0
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;此时正好满足 &lt;span class="math"&gt;\( p^*\)&lt;/span&gt;  点的位置二者导数相反且有一个  &lt;span class="math"&gt;\( \lambda &amp;gt; 0\)&lt;/span&gt; 的系数关系和约束条件  &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt;  ，函数  &lt;span class="math"&gt;\( L(x, \lambda)\)&lt;/span&gt; 叫做拉格朗日函数，  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  为拉格朗日乘子&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二种&lt;/strong&gt; ：图中绿色  &lt;span class="math"&gt;\( g'(x)\leq 0\)&lt;/span&gt; 部分，目标函数的最优解在约束条件范围内，此时就更好处理了，其实此时约束条件相当于没有，所以  &lt;span class="math"&gt;\( \lambda=0\)&lt;/span&gt; ，综合这两种情况可以推导出&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \bigtriangledown f(x) +\lambda\bigtriangledown g(x) = 0 &amp;amp; p^* point\\
    g(x)\leq 0 &amp;amp; origin \space s.t. \\
    \lambda \geq 0 &amp;amp; lagrange\space multiplier\\
    \lambda g(x) = 0 \end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;前三个很容易理解，上文中已经论证过了，但是  &lt;span class="math"&gt;\( \lambda g(x) = 0\)&lt;/span&gt;  这是个什么玩意？其实该等式是上面第一种情况 &lt;span class="math"&gt;\( \lambda&amp;gt;0 \space \&amp;amp; \space g(x)=0\)&lt;/span&gt;  和第二种情况  &lt;span class="math"&gt;\(\lambda=0 \space \&amp;amp; \space g(x)&amp;lt;0\)&lt;/span&gt;  整合出来的，这几个不等式就是 &lt;strong&gt;KKT(Karush-Kuhn-Tucker)条件&lt;/strong&gt; ，这里需要注意的是： &lt;strong&gt;原优化条件是对可行解的约束，KKT条件是对最优解的约束&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此时针对最初的SVM损失函数来说，就可以转化成这样（很多的书中拉格朗日乘子会用  &lt;span class="math"&gt;\( \alpha,\beta\)&lt;/span&gt; 表示，这里因为最初是用  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  的所以一下都用  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  来表示）&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{min}_{\theta,b}L(\theta, b, \lambda) = \frac{1}{2}||\theta||^2 + \sum_{i=1}^{N}{\lambda^{(i)}(1-y^{(i)}(\theta^Tx^{(i)}+b))} \\ s.t. \lambda^{(i)} \geq 0 ，\space i = 1,2...,N \\
$$&lt;/div&gt;
&lt;p&gt;至此虽然是找到了  &lt;span class="math"&gt;\( p^*\)&lt;/span&gt; 点，但是求解还是很不容易，因为目前构造拉格朗日函数目的是优化方向一样，但是如何让朗格朗日函数和原始问题的解是一样的呢？这里多出来的 &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;  应该怎么处理掉呢？&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;拉格朗日对偶&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;为了解决上面拉格朗日函数的问题，首先直接的想法就是 &lt;strong&gt;让这个函数和原始问题一致&lt;/strong&gt; ，然后求解这个拉格朗日函数就可以了，但是如今加进来一个 &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  结合 &lt;strong&gt;KKT条件&lt;/strong&gt; 如何让这两个问题一致呢？&lt;/p&gt;
&lt;p&gt;首先的想法就是 &lt;strong&gt;在可行解区域内，也就是符合原始问题的约束条件，拉格朗日函数与原始问题一样；在可行解区域外，让拉格朗日函数取到 &lt;span class="math"&gt;\( +\infty\)&lt;/span&gt;  ，这样一来在求解  &lt;span class="math"&gt;\( min\)&lt;/span&gt;  问题的时候，这俩问题就一样了&lt;/strong&gt;，所以可以构造函数，设其可行解区域为  &lt;span class="math"&gt;\( \Omega\)&lt;/span&gt; ，这里为了书写方便，不直接写SVM的形式，而是用更一般的形式目标函数为  &lt;span class="math"&gt;\( f(x)\)&lt;/span&gt;  ，约束条件为 &lt;span class="math"&gt;\( g(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\Theta_p(x) = \mathop{max}_{\lambda}L(x, \lambda) = f(x) +\mathop{max}_{\lambda}[\sum_{i=1}^{N}{\lambda^{(i)}g(x)}]\\
$$&lt;/div&gt;
&lt;p&gt;对于在可行解区域内  &lt;span class="math"&gt;\( x\in\Omega\)&lt;/span&gt;  ，根据  &lt;span class="math"&gt;\( \lambda^{(i)}\geq0\)&lt;/span&gt; ，  &lt;span class="math"&gt;\( g(x^{(i)})\leq0\)&lt;/span&gt;  可得&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{max}_{\lambda}[\sum_{i=1}^{N}{\lambda^{(i)}g(x)}]=0\\
$$&lt;/div&gt;
&lt;p&gt;如果不在可行解区域内  &lt;span class="math"&gt;\( x\notin\Omega\)&lt;/span&gt;  ，则至少有1组约束条件没有满足即  &lt;span class="math"&gt;\(
g_j(x)&amp;gt;0\)&lt;/span&gt;  ，这时候调整  &lt;span class="math"&gt;\( \lambda_j&amp;gt;0\)&lt;/span&gt;  ，取  &lt;span class="math"&gt;\( +\infty\)&lt;/span&gt; 就可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{max}_{\lambda}[\sum_{i=1}^{N}{\lambda^{(i)}g(x)}]=+\infty\\
$$&lt;/div&gt;
&lt;p&gt;此时我们构造的函数&lt;/p&gt;
&lt;div class="math"&gt;$$\Theta_p(x) = \mathop{max}_{\lambda}L(x, \lambda) =f(x) + \mathop{max}_{\lambda}[\sum_{i=1}^{N}{\lambda^{(i)}g(x)}]
$$&lt;/div&gt;
&lt;p&gt;就与原始问题等价了，原始问题的约束条件也就没有了，然后再最小化  &lt;span class="math"&gt;\( \Theta_p(x)\)&lt;/span&gt;  即 &lt;span class="math"&gt;\( min\space\Theta_p(x)\)&lt;/span&gt;  ，注意此时问题为 &lt;strong&gt;极小极大问题&lt;/strong&gt; ，拉格朗日对偶性就是可以把此时的&lt;strong&gt;极小极大问题转化为极大极小问题&lt;/strong&gt; （此部分的证明个人水平有限，没看懂o(╯□╰)o）即 &lt;strong&gt;原始问题的对偶问题如下&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathop{max}_{\lambda}\mathop{min}_{\theta, b}L(\theta, b,\lambda)=\frac{1}{2}||\theta||^2+\sum_{i=1}^{N}{\lambda^{(i)}(1-y^{(i)}(\theta^Tx^{(i)}+b))}\\
$$&lt;/div&gt;
&lt;p&gt;这样就先求解  &lt;span class="math"&gt;\( min\)&lt;/span&gt;  再求解  &lt;span class="math"&gt;\( max\)&lt;/span&gt;  ，求解  &lt;span class="math"&gt;\(\mathop{min}_{\theta, b}L(\theta, b, \lambda)\)&lt;/span&gt;  即 &lt;strong&gt;对 &lt;span class="math"&gt;\( \theta,b\)&lt;/span&gt; 求导为 &lt;/strong&gt; &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  &lt;strong&gt;求得&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \frac{\partial L}{\partial \theta} = \theta -\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}x^{(i)}}=0\\
    \frac{\partial L}{\partial b}=-\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}}=0
\end{cases}\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \theta=\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}x^{(i)}}\\
    \sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}}=0
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;将上式带入到原始问题的对偶问题中，此部分可能看起来比较麻烦，但是并不难，带入式子即可，这里的  &lt;span class="math"&gt;\( i,j\)&lt;/span&gt; 也只是为了区分而已，实际上并没什么区别&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \mathop{min}_{\theta, b}L(\theta, b, \lambda) &amp;amp;=\frac{1}{2}||\theta||^2+\sum_{i=1}^{N}{\lambda^{(i)}(1-y^{(i)}(\theta^Tx^{(i)}+b))}\\
    &amp;amp;=\frac{1}{2}||\theta||^2-\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}\theta^Tx^{(i)}}-b\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}+\sum_{i=1}^{N}{\lambda^{(i)}}}\\
    &amp;amp;=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\lambda^{(i)}\lambda^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}-\sum_{i=1}^{N}\sum_{j=1}^{N}{\lambda^{(i)}\lambda^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}+\sum_{i=1}^{N}{\lambda^{(i)}}\\
    &amp;amp;=\sum_{i=1}^{N}{\lambda^{(i)}}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\lambda^{(i)}\lambda^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;考虑上式的 &lt;strong&gt;极大化取负号变成最小化&lt;/strong&gt; ，结合对  &lt;span class="math"&gt;\( \theta,b\)&lt;/span&gt;  求导为  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;
和  &lt;span class="math"&gt;\( \lambda\geq0\)&lt;/span&gt;  就可以转化成，注意这里新增了一个约束条件为对  &lt;span class="math"&gt;\( b\)&lt;/span&gt;
求导为  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  时候的条件  &lt;span class="math"&gt;\(\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}}=0\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    min \space &amp;amp; \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}{\lambda^{(i)}\lambda^{(j)}y^{(i)}y^{(j)}x^{(i)}x^{(j)}}-\sum_{i=1}^{N}{\lambda^{(i)}}\\
    s.t. \space &amp;amp;\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}}=0\\
    \space &amp;amp; \lambda^{(i)}\geq0，i=1,2,\cdots,N
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;考虑  &lt;span class="math"&gt;\( \theta=\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}x^{(i)}}\)&lt;/span&gt;  ，假如
&lt;span class="math"&gt;\( \lambda^*=(\lambda^{*(1)},\lambda^{*(2)},...,\lambda^{*(l)})^T\)&lt;/span&gt;
是对偶问题的解，那么肯定至少有1个  &lt;span class="math"&gt;\( \lambda^{*(j)}&amp;gt;0\)&lt;/span&gt;  使得  &lt;span class="math"&gt;\(y^{(j)}(\theta^*\cdot x^{(j)}+b^*)-1=0\)&lt;/span&gt;  ，因为 &lt;strong&gt;间隔上至少存在1个支持向量&lt;/strong&gt; ，同时考虑 &lt;span class="math"&gt;\( (y^{(j)})^2=1，y^{(i)}\in{-1，+1}\)&lt;/span&gt;  则有&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp;y^{(j)}(\theta^*\cdot x^{(j)}+b^*)-1=0\\
    &amp;amp;y^{(i)}(\sum_{i=1}^{N}{\lambda^{(i)}y^{(i)}x^{(i)}x^{(j)}+b^*})=(y^{(j)})^2\\
    &amp;amp;b^*=y^{(j)}-\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}(x^{(i)}\cdot x^{(j)})}
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;至此  &lt;span class="math"&gt;\( \theta^*，b^*\)&lt;/span&gt;  都已经求得&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \theta^*=\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}x^{(i)}}\\
    b^*=y^{(j)}-\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}(x^{(i)}\cdot x^{(j)})}
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;所以分类决策函数就可以写成为如下形式，注意到这里还有前面的推导中  &lt;span class="math"&gt;\( (x^{(i)}\cdot x^{(j)})\)&lt;/span&gt;  都是&lt;strong&gt;单独括起来的&lt;/strong&gt; ，其实是表示SVM在分类决策函数中只 &lt;strong&gt;依赖输入 &lt;span class="math"&gt;\( x\)&lt;/span&gt;  和训练样本的内积&lt;/strong&gt;，这也是为什么对偶式更直接引出核函数，核函数所做的事情，就是把这个内积映射到高维空间中从而把线性不可分变为线性可分  &lt;/p&gt;
&lt;div class="math"&gt;$$
f(x)=sign(\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}(x\cdot x^{(i)})+b^*})\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;具体例子&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;通过上面的一通论证已经对SVM的对偶问题有了大概的理解，为了更清晰的看到是如何计算的，这里举个例子（例子来源李航老师的&lt;a href="https://book.douban.com/subject/33437381/"&gt;《统计学习方法 第二版》&lt;/a&gt;）分别计算原始问题和对偶问题&lt;/p&gt;
&lt;p&gt;有3个样本点分别为  &lt;span class="math"&gt;\( x^{(1)}=(3,3)^T; x^{(2)}=(4,3)^T;x^{(3)}=(1,1)^T\)&lt;/span&gt;
，其中  &lt;span class="math"&gt;\( x^{(1)},x^{(2)}\)&lt;/span&gt;  为正例，  &lt;span class="math"&gt;\( x^{(3)}\)&lt;/span&gt; 为负例，求这样最大间隔分离超平面，作图如下&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm2_example.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先求解原始问题&lt;/strong&gt; ，这里为了书写公式方便，将上角标  &lt;span class="math"&gt;\( i\)&lt;/span&gt;  挪到下角标处&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \mathop{min}_{\theta,b} \space &amp;amp;\frac{1}{2}(\theta_1^2+\theta_2^2)\\
    s.t.\space&amp;amp;3\theta_1+3\theta_2+b\geq1\\
    &amp;amp;4\theta_1+3\theta_2+b\geq1\\
    &amp;amp;-\theta_1-\theta2-b\geq1
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;可解得  &lt;span class="math"&gt;\( \theta_1=\theta_2=\frac{1}{2}，b=-2\)&lt;/span&gt;  时取得最小值，所以最大间隔超平面如下，其中 &lt;span class="math"&gt;\( x^{(1)},x^{(3)}\)&lt;/span&gt;  为支持向量&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{1}{2}x_1+\frac{1}{2}x_2-2=0\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;然后再求解对偶问题&lt;/strong&gt; ，  $ \theta^&lt;em&gt; , b^&lt;/em&gt; $  都与  $ \lambda^&lt;em&gt; $ 相关，则先求解  $ \lambda^&lt;/em&gt; $  ，得到了  $ \lambda^* $ 也就求得了超平面参数&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation} \begin{aligned}
    \mathop{min}_{\lambda} \space &amp;amp;\frac{1}{2}\sum_{i-1}^{N}\sum_{j=1}^{N}{\lambda_i \lambda_j y_i y_j (x_i\cdot x_j)}-\sum_{i=1}^{N}{\lambda_i}\\
    &amp;amp;=\frac{1}{2}(18\lambda_1^2+25\lambda_2^2+2\lambda_3^2+42\lambda_1 \lambda_2-12\lambda_1 \lambda_3-14\lambda_2 \lambda_3)-\lambda_1-\lambda_2-\lambda_3\\
    s.t.\space&amp;amp;\lambda_1+\lambda_2-\lambda_3=0\\
    &amp;amp;\lambda_i\geq0，i=1,2,3
\end{aligned} \end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( \lambda_3=\lambda_1+\lambda_2\)&lt;/span&gt; 带入目标函数中&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    J(\lambda_1, \lambda_2) &amp;amp;= 4\lambda_1^2+\frac{13}{2}\lambda_2^2+10\lambda_1 \lambda_2-2\lambda_1-2\lambda_2\\
    \Rightarrow \space&amp;amp; \begin{cases} \frac{\partial J}{\partial \lambda_1}=8\lambda_1+10\lambda_2-2=0\\
    \frac{\partial J}{\partial \lambda_2}=13\lambda_2+10\lambda_1-2=0 \end{cases}
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;可以解出  &lt;span class="math"&gt;\( J(\lambda_1, \lambda_2)\)&lt;/span&gt;  在点  &lt;span class="math"&gt;\((\frac{3}{2},-1)^T\)&lt;/span&gt;  处取到极值，但是 &lt;strong&gt;注意到这里的 &lt;span class="math"&gt;\( -1\)&lt;/span&gt;  不满足约束条件 &lt;/strong&gt; &lt;span class="math"&gt;\( \lambda_2\geq0\)&lt;/span&gt;  ，所以 &lt;strong&gt;此时最小值就不在边界内而是在边界上&lt;/strong&gt; ，就要分别考虑 &lt;span class="math"&gt;\( \lambda_1,\lambda_2\)&lt;/span&gt;  为  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  的情况&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    J(0, \frac{2}{13})=-\frac{2}{13} \space,for \space \lambda_1=0\\
    J(\frac{1}{4},0)=-\frac{1}{4} \space\space\space\space, for \space \lambda_2=0
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;所以  &lt;span class="math"&gt;\( (\lambda_1, \lambda_2, \lambda_3)=(\frac{1}{4}, 0,\frac{1}{4})^T\)&lt;/span&gt;  时对偶式取到极小值，通过拉格朗日乘子的特点就可以知道  &lt;span class="math"&gt;\( x^{(1)},x^{(3)}\)&lt;/span&gt; 为支持向量， &lt;strong&gt;因为该拉格朗日乘子不为 &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  所以约束条件  &lt;span class="math"&gt;\( g(x)=0\)&lt;/span&gt; ，所以该点就是在间隔边界上的 &lt;/strong&gt; ，然后再带入前面的  $ \theta^&lt;em&gt; , b^&lt;/em&gt; $  求得&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \theta^*&amp;amp;=\sum_{i=1}^{N}{\lambda^{*(i)}y^{(i)}x^{(i)}}&amp;amp;=\frac{1}{4}\times1\times(3,3)^T+0\times1\times(4,3)^T-\frac{1}{4}\times1\times(1,1)^T&amp;amp;=(\frac{1}{2},\frac{1}{2})^T\\
    b^*&amp;amp;=y^{(1)}-\sum_{i=1}^{N}{\lambda^{*(1)}y^{(i)}(x^{(i)}\cdot x^{(1)})} &amp;amp;=1-(\frac{1}{4}\times1\times18+0\times1\times21-\frac{1}{4}\times1\times6)&amp;amp;=-2
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;与求解原始问题的一样的&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;综上所述，硬间隔线性可分SVM对于给定的线性可分训练数据集，可以首先求解原始问题的对偶形式  &lt;span class="math"&gt;\( \lambda^*\)&lt;/span&gt; ，再计算分类决策函数的参数  &lt;span class="math"&gt;\( \theta^*,b^*\)&lt;/span&gt;  ，从而解决问题。但是截止到目前为止，都是&lt;strong&gt;假设数据的硬间隔线性可分的&lt;/strong&gt; ，就说硬间隔是客观存在的，而在真实世界中往往有很多噪声，&lt;strong&gt;比如在间隔中存在杂乱的正例和负例，此时是找不出硬间隔的，这时候怎么办？&lt;/strong&gt; 又或者 &lt;strong&gt;数据本身就不是线性可分的又该如何？&lt;/strong&gt; 这些问题在下一次 &lt;a href="https://reed-qu.github.io/svmsan-cong-ruan-jian-ge-dao-he-han-shu.html"&gt;SVM(三) &lt;/a&gt; 中讨论。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>SVM(一)从感知机到SVM损失函数</title><link href="https://reed-qu.github.io/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html" rel="alternate"></link><published>2020-01-24T16:12:11+08:00</published><updated>2020-01-24T16:12:11+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/svmyi-cong-gan-zhi-ji-dao-svmsun-shi-han-shu.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在做二分类任务的时候，往往可采用的方法有很多，比如 &lt;a href="https://reed-qu.github.io/dui-shu-ji-lu-hui-gui-luo-ji-hui-gui.html"&gt; 对数几率回归 &lt;/a&gt; 或者树模型(往往是 &lt;a href="https://reed-qu.github.io/ji-cheng-xue-xi-sui-ji-sen-lin-adaboost.html"&gt; 随机森林 &lt;/a&gt; ， &lt;a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"&gt; GBDT&lt;/a&gt; 这一类集成模型)，当然还有很多种办法， &lt;strong&gt;&lt;a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8"&gt; 感知机&lt;/a&gt;&lt;/strong&gt; 就是其中的比较直观和简单的一种，感知机的原理很简单，考虑在特种空间中的 &lt;strong&gt;线性可分 …&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在做二分类任务的时候，往往可采用的方法有很多，比如 &lt;a href="https://reed-qu.github.io/dui-shu-ji-lu-hui-gui-luo-ji-hui-gui.html"&gt; 对数几率回归 &lt;/a&gt; 或者树模型(往往是 &lt;a href="https://reed-qu.github.io/ji-cheng-xue-xi-sui-ji-sen-lin-adaboost.html"&gt; 随机森林 &lt;/a&gt; ， &lt;a href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html"&gt; GBDT&lt;/a&gt; 这一类集成模型)，当然还有很多种办法， &lt;strong&gt;&lt;a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8"&gt; 感知机&lt;/a&gt;&lt;/strong&gt; 就是其中的比较直观和简单的一种，感知机的原理很简单，考虑在特种空间中的 &lt;strong&gt;线性可分&lt;/strong&gt; 的数据集，是能够找到一条 &lt;strong&gt;分类超平面&lt;/strong&gt;把两类数据给正确区分开的，这个不是这里要讨论的重点，这里要讨论的是这种很显然有 &lt;strong&gt;无数种超平面使两分类正确分类的时候如何找到”最好“的那一条&lt;/strong&gt;，有了“最好”的限制条件，就成为了这里要说的 &lt;strong&gt;SVM&lt;/strong&gt; &lt;code&gt;Support Vector Machine&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;感知机&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;感知机是 &lt;strong&gt;二分类的线性分类模型&lt;/strong&gt; ，感知机要做的事是在训练数据中找到一个 &lt;strong&gt;超平面&lt;/strong&gt;，将数据分为正例和负例两类，其输入就是数据的特征向量，这里简单的说一下感知机的过程，主要是为了引出后面的SVM，这里只考虑线性可分的情况&lt;/p&gt;
&lt;p&gt;首先给定数据集  &lt;span class="math"&gt;\( D={(x^{(1)}, y^{(1)}), (x^{(2)},y^{(2)}),\cdots,(x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\( x^{(i)}\in R^n\)&lt;/span&gt;，&lt;span class="math"&gt;\( y^{(i)}\in {-1, +1\\}\)&lt;/span&gt;  &lt;span class="math"&gt;\( i = 1,2\cdots,N\)&lt;/span&gt;，如下图所示的红色和蓝色的点，这里不是回归问题中的拟合一条直线去逼近这些数据点，而是找到一条直线将两类数据划分开，考虑二维空间中直线的公式为 &lt;span class="math"&gt;\( y=ax+b\)&lt;/span&gt;  ，这里  &lt;span class="math"&gt;\( x,y\)&lt;/span&gt;  是可以看成是数据点的两个维度可以转化成 &lt;span class="math"&gt;\( x_1,x_2\)&lt;/span&gt;  ，于是直线就可以改写成  &lt;span class="math"&gt;\( ax_1-x_2+b=0\)&lt;/span&gt;  ，可以更进一步的写成&lt;strong&gt;向量&lt;/strong&gt; 的形式&lt;/p&gt;
&lt;div class="math"&gt;$$
[a, -1]^T\bullet [x_1,x_2]+b=\theta^Tx+b=0\\
$$&lt;/div&gt;
&lt;p&gt;此时感知机模型表示为  &lt;span class="math"&gt;\( y = sign(\theta^T\cdot x + b)\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\(sign\)&lt;/span&gt;  为 &lt;strong&gt;符号函数&lt;/strong&gt; ，此时定义感知机的损失函数之前需要明确以下几点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\( \theta^Tx+b=0\)&lt;/span&gt;  这条直线将数据分为两类，那么就满足  &lt;span class="math"&gt;\( theta^Tx+b&amp;gt;0\)&lt;/span&gt;  为正例输出  &lt;span class="math"&gt;\( y=+1\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\( \theta^Tx+b&amp;lt;0\)&lt;/span&gt;  为负例输出  &lt;span class="math"&gt;\( y=-1\)&lt;/span&gt;  ，此时对于正确分类的数据来说  &lt;span class="math"&gt;\( y^{(i)}(\theta^Tx^{(i)} + b) &amp;gt;0\)&lt;/span&gt;  ，分类错误的数据则为  &lt;span class="math"&gt;\( y^{(i)}(\theta^Tx^{(i)}+b)&amp;lt;0\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;数据点到直线的距离  &lt;span class="math"&gt;\( \frac{|\theta^Tx+b|}{||\theta||_2}\)&lt;/span&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以定义感知机的损失函数可以 &lt;strong&gt;让误分类的数据到直线的距离和最小&lt;/strong&gt; ，因为是针对误分类数据的，所以损失函数这么写&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \sum_{i=1}^{M}{\frac{-y^{(i)}(\theta^Tx^{(i)}+b)}{||\theta||_2}} = -\frac{1}{||\theta||_2}\sum_{i=1}^{M}{y^{(i)}(\theta^Tx^{(i)}+b)}\\
$$&lt;/div&gt;
&lt;p&gt;这里  &lt;span class="math"&gt;\( M\)&lt;/span&gt;  为误分类点的个数，同时发现  &lt;span class="math"&gt;\( \frac{1}{||\theta||_2}\)&lt;/span&gt;
可以不考虑，因为这里是针对 &lt;strong&gt;误分类&lt;/strong&gt; 的数据构建的损失函数，即在没有误分类的情况下损失最小为  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  ，此时 &lt;span class="math"&gt;\( \theta,b\)&lt;/span&gt;  取任意值都可以，且  &lt;span class="math"&gt;\( ||\theta||_2\)&lt;/span&gt;  是 &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  的L2范数，也就是损失函数的一个 &lt;strong&gt;常量因子&lt;/strong&gt; ，所以感知机的损失函数和  &lt;span class="math"&gt;\(\theta, b\)&lt;/span&gt;  的偏导数即是&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    J(\theta,b) &amp;amp;= -\sum_{i=1}^{M}{y^{(i)}(\theta^Tx^{(i)}+b)}\\    \frac{\partial J}{\partial \theta} &amp;amp;= -\sum_{i=1}^{M}{y^{(i)}x^{(i)}}\\
    \frac{\partial J}{\partial b} &amp;amp;= -\sum_{i=1}^{M}{y^{(i)}} \end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;有了损失函数和  &lt;span class="math"&gt;\( \theta, b\)&lt;/span&gt;  的偏导，就可以利用 &lt;a href="https://reed-qu.github.io/pi-liang-ti-du-xia-jiang.html"&gt; 梯度下降&lt;/a&gt;来对其进行迭代求解，这里不再赘述了，感知机说到这里主要有一个问题，就是感知机的这个超平面是有无数个的，比如下图所示的线性可分的数据，在两条黑色虚线的间隔中，可以画出无数条直线能够完美的将数据划分开，&lt;strong&gt;那么是否能找到最好的一条呢？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm1_preceptron.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;从图上可以直观的看出来，蓝色和绿色的直线虽然也能将数据划分开，但是都或多或少的向某一边“ &lt;strong&gt;&lt;em&gt;倾斜&lt;/em&gt; &lt;/strong&gt;”，所以这里要找的内条“最好”的直线是指这条线是 &lt;strong&gt;以“最可靠”的方式来划分&lt;/strong&gt; 的，&lt;strong&gt;不但将训练数据正确划分，也能够对未知数据有足够的可信度将其分开&lt;/strong&gt; ，从图中也显然能观察出来间隔中间红色的直线要好于另外两条， &lt;strong&gt;SVM&lt;/strong&gt;就是找到这条红线的一种方法。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;硬间隔线性可分SVM&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;首先只讨论数据集线性可分的情况，给定数据集&lt;span class="math"&gt;\(D={(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\cdots,(x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\( x^{(i)}\in R^n\)&lt;/span&gt;  ，&lt;span class="math"&gt;\( y^{(i)}\in {-1, +1}\)&lt;/span&gt;  ，根据前面对感知机的了解，要同时满足两个条件才能确定这个超平面&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;正确分类正例和负例，且要让数据，分布在间隔上或者间隔外，因为如果有数据还在间隔内，那显然间隔的划分的是有问题的 &lt;/li&gt;
&lt;li&gt;两类数据中间的 &lt;strong&gt;间隔要足够大&lt;/strong&gt; ，这样间隔中间的这条线才是足够可靠的 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结合下面的图来具体的说一下SVM的优化方向&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/svm1_hard_margin.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;根据SVM正负例都在间隔上或者间隔外这一特点，如果数据被正确分类的话就会有数据点的 &lt;strong&gt;几何间隔&lt;/strong&gt; 符合&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \frac{\theta^Tx^{(i)}+b}{||\theta||} \geq d, &amp;amp; y^{(i)} = 1\\    \frac{\theta^Tx^{(i)}+b}{||\theta||} \leq d, &amp;amp; y^{(i)} = -1
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;不等式左右同时除以  &lt;span class="math"&gt;\( d\)&lt;/span&gt;  ，不等式变换为&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \frac{\theta^Tx^{(i)}+b}{||\theta||d} \geq 1, &amp;amp;y^{(i)} = 1\\    \frac{\theta^Tx^{(i)}+b}{||\theta||d} \leq 1, &amp;amp; y^{(i)} = -1
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;上式中主要关注  &lt;span class="math"&gt;\( \frac{\theta^T}{||\theta||d}\)&lt;/span&gt;  和  &lt;span class="math"&gt;\(\frac{b}{||\theta||d}\)&lt;/span&gt;  ，这时候可以发现， &lt;strong&gt;事情本质并没有发生什么变化，这两个值只是在&lt;/strong&gt; &lt;span class="math"&gt;\(\theta^T,b\)&lt;/span&gt;  &lt;strong&gt;的基础上对其数值大小进行了一定的缩放&lt;/strong&gt; ，就仍然可以写成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \theta^Tx^{(i)}+b \geq 1, &amp;amp; y^{(i)} = 1\\
    \theta^Tx^{(i)}+b \leq 1, &amp;amp; y^{(i)} = -1
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;综合上面2种情况将  &lt;span class="math"&gt;\( y^{(i)}\)&lt;/span&gt;  乘进去就可以变成一个不等式  &lt;span class="math"&gt;\(y^{(i)}(\theta^Tx^{(i)}+b)\geq1\)&lt;/span&gt;  ，这就是SVM优化问题中的 &lt;strong&gt;约束条件&lt;/strong&gt; ，下面来讨论最大间隔这件事儿&lt;/p&gt;
&lt;p&gt;首先要知道 &lt;strong&gt;最大几何间隔的表达方式&lt;/strong&gt; ，图中看的更直观一些，首先这个最大间隔到底能有多大，其实是取决于在间隔边界上这些数据点，这些点叫做&lt;strong&gt;支持向量&lt;/strong&gt; ，SVM的超平面也是主要取决于这些支持向量，几何间隔计算也不困难，定义几何间隔为  &lt;span class="math"&gt;\( d\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
d=\frac{|\theta^Tx+b|}{||\theta||}=\frac{1}{||\theta||}\\
$$&lt;/div&gt;
&lt;p&gt;所以想要最大间隔的 &lt;strong&gt;SVM的损失函数即是这样定义的，在满足约束条件的同时最大化几何间隔&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp;\mathop{max}_{\theta}\frac{1}{||\theta||}\\
    &amp;amp;s.t. \space y^{(i)}(\theta^Tx^{(i)}+b)\geq1
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;其中这里 &lt;span class="math"&gt;\( \mathop{max}_{\theta}\frac{1}{||\theta||}\)&lt;/span&gt;  表示最大化，效果也等价于 &lt;span class="math"&gt;\( \mathop{min}_{\theta} \frac{1}{2}||\theta||^2\)&lt;/span&gt;  ，&lt;span class="math"&gt;\( \frac{1}{2}\)&lt;/span&gt;  也容易理解，是为了方便求导计算，并不影响  &lt;span class="math"&gt;\( \theta\)&lt;/span&gt; 的优化，所以至此硬 &lt;strong&gt;间隔线性可分SVM&lt;/strong&gt; 的损失表示就为&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp;\mathop{min}_{\theta}\frac{1}{2}||\theta||^2\\
    &amp;amp;s.t. \space y^{(i)}(\theta^Tx^{(i)}+b)\geq1
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;但是这个损失函数并不是单独的一个式子，而是 &lt;strong&gt;有约束的&lt;/strong&gt; ( &lt;span class="math"&gt;\( s.t.\)&lt;/span&gt; 为 &lt;em&gt;subject to的缩写&lt;/em&gt;)的损失函数，其意思呢就是在  &lt;span class="math"&gt;\( s.t. \space y^{(i)}\theta^Tx^{(i)}+b)\geq1\)&lt;/span&gt; 这个 &lt;strong&gt;可行解空间中&lt;/strong&gt; 去优化 &lt;span class="math"&gt;\( \mathop{min}_{\theta}\frac{1}{2}||\theta||^2\)&lt;/span&gt;  ，这部分在下一篇 &lt;a href="https://reed-qu.github.io/svmer-la-ge-lang-ri-dui-ou-shi.html"&gt; SVM(二)&lt;/a&gt; 中主要讨论。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>从GBDT到XGBoost</title><link href="https://reed-qu.github.io/cong-gbdtdao-xgboost.html" rel="alternate"></link><published>2020-01-24T15:42:53+08:00</published><updated>2020-01-24T15:42:53+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/cong-gbdtdao-xgboost.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;这次继续聊Boosting模型，上一篇聊了AdaBoost，AdaBoost的核心思想是通过提升错分的数据权重，来更关注错分的数据，同时根据每次迭代的正确率来给当前的弱模型一个权重，这个弱模型准确率越高，则这个弱模型的权重就越大，这是合理的，但是其实这样还是不够直接，更直接的做法是比如说第一次训练了一个弱模型，这个弱模型肯定不会是100%完美的，肯定会存在一定的误差(残差)，此时更直接的想法就是我再 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;这次继续聊Boosting模型，上一篇聊了AdaBoost，AdaBoost的核心思想是通过提升错分的数据权重，来更关注错分的数据，同时根据每次迭代的正确率来给当前的弱模型一个权重，这个弱模型准确率越高，则这个弱模型的权重就越大，这是合理的，但是其实这样还是不够直接，更直接的做法是比如说第一次训练了一个弱模型，这个弱模型肯定不会是100%完美的，肯定会存在一定的误差(残差)，此时更直接的想法就是我再&lt;strong&gt;训练一个模型来把这部分误差学习出来&lt;/strong&gt;，然后两个模型叠加就可以了，但是此时肯定还不是完美的，不要紧，下一次迭代的时候再学习剩下部分的误差，从而让误差越来越小，这就是今天要聊的GBDT的所做的事情。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;梯度提升树 GBDT&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;类似Boosting的思想，GBDT也是采用 &lt;strong&gt;分步迭代&lt;/strong&gt; 学习弱模型的方式来学习最终的模型的，但是这里是学习了一个弱模型之后，在不改变这个弱模型也不改变数据的权重(&lt;code&gt;AdaBoost&lt;/code&gt;)的基础上，学习另外一个弱模型，然后二者组合起来作为第二轮的模型，这样在不断学习并且模型不断累加的过程中让这些弱模型组合成表现更好的强模型&lt;/p&gt;
&lt;p&gt;假设现在有数据集  &lt;span class="math"&gt;\(D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(N)},y^{(N)})}\)&lt;/span&gt;  ，其中 &lt;span class="math"&gt;\( x^{(i)}\in R^n\)&lt;/span&gt;  ，在第一轮的时候初始化一个树结构模型：  &lt;span class="math"&gt;\( H_0(x) =0\)&lt;/span&gt;  ，就是说对每一个输入的  &lt;span class="math"&gt;\( x\)&lt;/span&gt;  都给同一个值，这里是  &lt;span class="math"&gt;\( 0\)&lt;/span&gt; 当然也可以是其他数字，比如 &lt;strong&gt;均值&lt;/strong&gt; 可能比  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  会更好一些，有了第一步那么后面就可以一步一步的迭代计算了，得到第 &lt;span class="math"&gt;\( m\)&lt;/span&gt;  轮的模型&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    H_0(x)&amp;amp;=0\\
    H_1(x)&amp;amp;=H_0(x)+T_1(x;\Theta)\\
    H_2(x)&amp;amp;=H_1(x)+T_2(x;\Theta)\\
    &amp;amp;\vdots\\
    H_m(x)&amp;amp;= H_{m-1}(x) +T_m(x;\Theta)\\
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( H_m(x)\)&lt;/span&gt;  为第  &lt;span class="math"&gt;\( m\)&lt;/span&gt;  的模型，  &lt;span class="math"&gt;\(H_{m-1}(x)\)&lt;/span&gt;  是上一轮的已知的模型，  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;  就是第  &lt;span class="math"&gt;\(m\)&lt;/span&gt;  轮需要加进来的树模型，是需要学习的，这里  &lt;span class="math"&gt;\( \Theta\)&lt;/span&gt;  表示这棵树的结构，包括 &lt;strong&gt;区域划分&lt;/strong&gt; 和&lt;strong&gt;各区域内的预测值&lt;/strong&gt; ，我们想要让这个  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt; 加进来之后模型变得更好也就是损失变得更小，我们就可以这么估计这个  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat\Theta_m=\mathop{argmin}_{\Theta_m}\sum_{i=1}^{N}{L(y^{(i)},H_{m-1}(x^{(i)})+T_m(x^{(i)};\Theta))}\\
$$&lt;/div&gt;
&lt;p&gt;这样优化这个函数，来求得  &lt;span class="math"&gt;\( \hat\Theta_m\)&lt;/span&gt;  也就是第  &lt;span class="math"&gt;\( m\)&lt;/span&gt;  棵树
&lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;  的参数，确定了参数也就确定了加进来的这棵树是什么样的了，就得到了第 &lt;span class="math"&gt;\( m\)&lt;/span&gt;  轮的模型，下一轮再加进来一棵树直到最后损失收敛，这里  &lt;span class="math"&gt;\( L(a, b)\)&lt;/span&gt;
表示二者的某种损失函数  &lt;span class="math"&gt;\( Loss Function\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;回归问题中一般用 &lt;strong&gt;均方误差&lt;/strong&gt; 损失&lt;/p&gt;
&lt;div class="math"&gt;$$
L(y, f(x)) = \frac{1}{2}(y-f(x))^2\\
$$&lt;/div&gt;
&lt;p&gt;分类问题中一般用  &lt;span class="math"&gt;\( logistic\)&lt;/span&gt;  损失&lt;/p&gt;
&lt;div class="math"&gt;$$
L(y, f(x)) = f(x)ln(1+e^{-f(x)})+(1-f(x))ln(1+e^{f(x)})\\
$$&lt;/div&gt;
&lt;p&gt;这里以均方误差损失为例，上式的损失函数就是&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    L(y^{(i)},H_{m-1}(x^{(i)})+T_m(x^{(i)};\Theta))&amp;amp;=(y^{(i)}-H_{m-1}(x^{(i)})-T_m(x^{(i)})^2\\
    &amp;amp;=(residual-T_m(x^{(i)}))^2
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;residual&lt;/em&gt;&lt;/strong&gt; 就是所谓的残差  &lt;span class="math"&gt;\( y^{(i)}-H_{m-1}(x^{(i)})\)&lt;/span&gt; ，即本轮学习之后仍然存在的误差，本轮需要加进去的这棵树就是照着残差去学习出来的树结构，其实也可以从梯度的角度来理解，针对平方损失函数 &lt;span class="math"&gt;\( L(y, f(x)) = \frac{1}{2}(y-f(x))^2\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\( f(x)\)&lt;/span&gt; 为某一轮结束时的累加起来的模型，此时应该沿着 &lt;strong&gt;损失的负梯度方向&lt;/strong&gt; 继续前进，也就是&lt;/p&gt;
&lt;div class="math"&gt;$$
-\frac{\partial L(y, f(x))}{\partial f(x)}=y-f(x)=residual \\
$$&lt;/div&gt;
&lt;p&gt;其实也是一样的，最终得到的模型就是  &lt;span class="math"&gt;\( H(x) = \sum_{m=1}^{M}{T_m(x;\Theta)}\)&lt;/span&gt;  ，将 &lt;span class="math"&gt;\( m\)&lt;/span&gt;  轮学习到的树模型累加在一起作为最终模型输出&lt;/p&gt;
&lt;p&gt;下图比较清晰的解释了这一过程， &lt;strong&gt;红色的✘&lt;/strong&gt; 是数据点，  &lt;span class="math"&gt;\( j\)&lt;/span&gt; 为找到的分割点(这里是一颗CART)，左右两部分分别的预测值就是 &lt;strong&gt;橙色的实折线&lt;/strong&gt; ，发现并没有拟合的特别好， &lt;strong&gt;绿色的虚线&lt;/strong&gt; 为预测值到真实值的误差，也就是我们所谓的残差，这时候就可以将绿色这部分的值作为另一颗CART训练的数据，最终形成 &lt;strong&gt;紫色的虚折线&lt;/strong&gt;，很明显紫色的线要比橙色的线做的要好&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/xgb_cart.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;但是 &lt;strong&gt;残差的这种形式只针对于回归问题中的平方损失函数&lt;/strong&gt;，如果换了其他损失函数就没这么简单了，但是计算方式和上面推导的过程是一样的，只不过不能简单是用残差来学习本轮的树模型，而是要沿着损失的负梯度 &lt;span class="math"&gt;\( -\frac{\partial L(y, f(x))}{\partial f(x)}\)&lt;/span&gt;  的方向来学习&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GBDT也有一些弊端&lt;/strong&gt; ，虽然有尚可的解决方案但是效果还是不是很好，主要有以下几点：&lt;/p&gt;
&lt;p&gt;1. 对于平方损失来说，对 &lt;strong&gt;噪声数据很敏感&lt;/strong&gt; ，因为此时损失为残差的平方，这样就加大了模型对这些噪声数据的关注度，容易过拟合，解决办法就是换一种损失函数，让损失不会过分放大，比如说 &lt;strong&gt;绝对值损失&lt;/strong&gt; &lt;span class="math"&gt;\( L(y, f(x)) = |y-f(x)|\)&lt;/span&gt;  ，或者 &lt;strong&gt;Huber损失&lt;/strong&gt; 来解决&lt;/p&gt;
&lt;div class="math"&gt;$$L(y, f(x)) =
\begin{cases}
    \frac{1}{2}(y-f(x))^2&amp;amp; \text{|y-f(x)|≤δ}\\
    \delta(|y-f(x)|-\frac{\delta}{2})&amp;amp; \text{|y-f(x)|&amp;gt;δ}
\end{cases}\\
$$&lt;/div&gt;
&lt;p&gt;2. 树模型”准确“的照着残差的方式去学习，很容易出现 &lt;strong&gt;Z形学习曲线&lt;/strong&gt; ，步长过大，让模型更不易收敛，解决的办法就是为  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt; 添加一个&lt;strong&gt;&lt;em&gt;shrinkage&lt;/em&gt;&lt;/strong&gt; ，变成  &lt;span class="math"&gt;\( \epsilon T_m(x;\Theta)\)&lt;/span&gt;  让步子小一点，这里  &lt;span class="math"&gt;\( \epsilon\)&lt;/span&gt;  类似于学习速率一般为一个很小的小数来控制每一次学习的幅度 &lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/xgb_z.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;针对GBDT的各种问题，陈天奇博士在论文中给出了更优的解决办法，就是 &lt;strong&gt;&lt;a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"&gt;XGBoost&lt;/a&gt;&lt;/strong&gt;，基于GBDT的思想做出了各种优化，比如算法层面添加&lt;strong&gt;正则化项&lt;/strong&gt; 让模型更简单，一定程度的避免过拟合，还有在损失函数上进行了 &lt;strong&gt;二阶泰勒展开&lt;/strong&gt; ，提升模型的精度，还有工程层面尽最大可能的&lt;strong&gt;并行处理&lt;/strong&gt; ，让训练效率更高，更有在特征工程层面上对 &lt;strong&gt;缺失值的处理&lt;/strong&gt; 优化等等，这些优化也让XGB在一些竞赛中取得了很好的成绩&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;加强版的GBDT XGBoost&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;XGB最终的模型和GBDT是一样的，为  &lt;span class="math"&gt;\( H(x) = \sum_{m=1}^{M}{T_m(x;
\Theta)},T_m\in F\)&lt;/span&gt;  ，也是基于加法的树模型，但是XGB的损失函数相较于GBDT略有不同，这里加了正则化项，在回归问题中正则化项一般有&lt;strong&gt;L1-norm(LASSO)&lt;/strong&gt; &lt;span class="math"&gt;\( \lambda||\theta||_1\)&lt;/span&gt;  和 &lt;strong&gt;L2-norm(Ridge)&lt;/strong&gt; &lt;span class="math"&gt;\( \lambda||\theta||_2\)&lt;/span&gt;  这里  &lt;span class="math"&gt;\( \theta\)&lt;/span&gt; 为各个特征的系数，在这里即为 &lt;strong&gt;树模型的结构复杂度&lt;/strong&gt; ，比如 &lt;strong&gt;叶子节点的数量&lt;/strong&gt; 和 &lt;strong&gt;树的深度&lt;/strong&gt; 还有 &lt;strong&gt;叶子节点中多个样本的L2-norm&lt;/strong&gt; ，XGB中第  &lt;span class="math"&gt;\( m\)&lt;/span&gt;  轮的损失函数就为&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss_m={\sum_{i=1}^{N}{l(y^{(i)},H_m(x^{(i)}))}}+\Omega(T_m)\\
$$&lt;/div&gt;
&lt;p&gt;其中  &lt;span class="math"&gt;\( \Omega(T_m)\)&lt;/span&gt; 就是XGB中的正则化项，代表本轮需要加进去的这个树模型的复杂度，这里我们同时考虑回归问题中的均方误差损失和  &lt;span class="math"&gt;\( H_m(x)= H_{m-1}(x) + T_m(x;\Theta)\)&lt;/span&gt;  ，可以进一步得出损失函数&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    Loss_m &amp;amp;=\sum_{i=1}^{N}{l(y^{(i)}, H_m(x^{(i)}))+\Omega(T_m(x;\Theta))}\\
    &amp;amp;=\sum_{i=1}^{N}{l(y^{(i)}, H_{m-1}(x^{(i)}) +T_m(x;\Theta))+\Omega(T_m(x;\Theta))}\\
    &amp;amp;=\sum_{i=1}^{N}{[(y^{(i)}-H_{m-1}(x^{(i)}))-T_m(x;\Theta)]^2}
+\Omega(T_m(x;\Theta))
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;此时其实就是加了 &lt;strong&gt;正则化项的残差&lt;/strong&gt; 拟合方式，如果换了其他损失这里还是不容易解决的，XGB的关键点就在这一处，XGB用了&lt;strong&gt;泰勒公式的二阶展开式来对损失近似求解&lt;/strong&gt; ，同时也使得XGB更通用，只要损失函数二阶可导都可以应用这种方法处理，用户甚至可以自己定义二阶可导的损失函数&lt;/p&gt;
&lt;p&gt;首先 &lt;strong&gt;泰勒公式&lt;/strong&gt; 的二阶展开式为&lt;/p&gt;
&lt;div class="math"&gt;$$
f(x+\Delta x) \simeq f(x) + f^{'}(x)\Delta x + \frac{1}{2!}f^{''}(x)\Delta x^2\\
$$&lt;/div&gt;
&lt;p&gt;其次需要解释的是  &lt;span class="math"&gt;\( l(y^{(i)},H_{m-1}(x^{(i)}) + T_m(x;\Theta))\)&lt;/span&gt;  中  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;  相较于从 &lt;span class="math"&gt;\( 1\)&lt;/span&gt;  到  &lt;span class="math"&gt;\( m-1\)&lt;/span&gt;  轮的所有树相加是很小的，所以泰勒展开式中的 &lt;span class="math"&gt;\( \Delta x\)&lt;/span&gt;  就是这里的  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;  ，而 &lt;span class="math"&gt;\( f(x)\)&lt;/span&gt;  对应这里的就是  &lt;span class="math"&gt;\( l(y^{(i)}, H_{m-1}(x^{(i)}))\)&lt;/span&gt; ，我们的损失函数就可以写成&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss_m\simeq\sum_{i=1}^{N}{[l(y^{(i)},H_{m-1}(x^{(i)}))+g^{(i)} T_m(x;\Theta)+\frac{1}{2}h^{(i)} T_m^2(x;\Theta)]}+\Omega(T_m(x;\Theta))\\
$$&lt;/div&gt;
&lt;p&gt;其中  &lt;span class="math"&gt;\( g\)&lt;/span&gt;  为一阶导，  &lt;span class="math"&gt;\( h\)&lt;/span&gt;  为二阶导，这里  &lt;span class="math"&gt;\(l(y^{(i)}, H_{m-1}(x^{(i)}))\)&lt;/span&gt;  为 &lt;strong&gt;常数项&lt;/strong&gt; 所以在损失函数中可以舍掉，主要关注  &lt;span class="math"&gt;\( g\)&lt;/span&gt; 和  &lt;span class="math"&gt;\( h\)&lt;/span&gt;  ，这里为了计算方便依旧 &lt;strong&gt;考虑平方损失函数&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
g = \frac{\partial^{'} l(y, H(x))}{\partial H(x)}=2(H(x)-y)\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
h = \frac{\partial^{''} l(y, H(x))}{\partial H(x)}=2\\
$$&lt;/div&gt;
&lt;p&gt;最终的需要关注的损失函数就是&lt;/p&gt;
&lt;div class="math"&gt;$$Loss_m\simeq\sum_{i=1}^{N}{[g^{(i)}T_m(x;\Theta)+\frac{1}{2}h^{(i)} T_m^2(x;\Theta)]}+\Omega(T_m(x;\Theta))\\
$$&lt;/div&gt;
&lt;p&gt;也就是说主要求出当前第  &lt;span class="math"&gt;\( m-1\)&lt;/span&gt;  轮的模型的一阶导数和二阶导数然后最优化  &lt;span class="math"&gt;\( Loss\)&lt;/span&gt;
就可以得到每一轮的  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后考虑正则化项  &lt;span class="math"&gt;\( \Omega(T_m(x;\Theta))\)&lt;/span&gt;  ，我们用 &lt;strong&gt;叶子节点的数量&lt;/strong&gt;
&lt;span class="math"&gt;\( T\)&lt;/span&gt;  ，和 &lt;strong&gt;每个叶子节点中的得分&lt;/strong&gt; &lt;span class="math"&gt;\( w\)&lt;/span&gt; (虽然一个叶子节点中有多个样本，但是是用一个值来作为这个叶子节点的输出的)来衡量模型的复杂度，以下图解引用陈天奇博士PPT中的图解，PPT中表示为 &lt;span class="math"&gt;\(\Omega(f_t)\)&lt;/span&gt;  &lt;/p&gt;
&lt;div class="math"&gt;$$
\Omega(T_m(x;\Theta)) = \gamma T+\frac{1}{2}\lambda \sum_{j=1}^{T}{w_j^2}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/xgb_regularization.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;这里  &lt;span class="math"&gt;\( \gamma\)&lt;/span&gt;  和  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt; 就是一个因子系数，表示正则化项中更关注哪一部分， &lt;strong&gt;用 &lt;span class="math"&gt;\( I_j\)&lt;/span&gt;  表示第  &lt;span class="math"&gt;\( j\)&lt;/span&gt; 个叶子节点中的样本集合 (  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  对于CART来说就是从根节点开始不断的找到收益最大的分裂点 &lt;span class="math"&gt;\( s\)&lt;/span&gt;  来二划分&lt;/strong&gt; )，此时损失函数就可以转化成下面这样&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    Loss_m&amp;amp; \simeq \sum_{i=1}^{N}{[g^{(i)}T_m(x;\Theta)+\frac{1}{2}h^{(i)} T_m^2(x;\Theta)]}+\gamma T+\frac{1}{2}\lambda \sum_{j=1}^{T}{w_j^2}\\
    &amp;amp; \simeq \sum_{j=1}^{T}{[(\sum_{i\in I_j}^{}{g^{(i)}})w_j+\frac{1}{2}(\sum_{i\in I_j}^{}{h^{(i)}+\lambda})w_j^2]+\gamma T} \end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;然后继续定义  &lt;span class="math"&gt;\( \sum_{i\in I_j}^{}{g^{(i)}} = G_j\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\(\sum_{i\in I_j}^{}{h^{(i)}} = H_j\)&lt;/span&gt;  ，最终等式就可以写成&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss_m\simeq\sum_{j=1}^{T}{\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2 \right]+\gamma T}\\
$$&lt;/div&gt;
&lt;p&gt;此时令  &lt;span class="math"&gt;\( Loss\)&lt;/span&gt;  的导数为  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;  即可求得&lt;/p&gt;
&lt;div class="math"&gt;$$
w_j=-\frac{G_j}{H_j+\lambda}
$$&lt;/div&gt;
&lt;p&gt;此时  &lt;span class="math"&gt;\( Loss\)&lt;/span&gt;  就可以简洁为&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss\simeq-\frac{1}{2}\sum_{j=1}^{T}{\frac{G_j^2}{H_j+\lambda}+\gamma T}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/xgb_loss.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;到此为止一切都做的很不错，但是有一个问题就是  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  ，我们并不知道这棵树是怎么划分的，也就是说第 &lt;span class="math"&gt;\( j\)&lt;/span&gt;  个叶子节点我们并不知道，这时候 &lt;strong&gt;暴力穷举树结构肯定是不可取&lt;/strong&gt; 的，就需要根据我们的损失函数，&lt;strong&gt;寻找收益最大的分裂点来进行划分&lt;/strong&gt; ，具体的步骤是这样的，这里定义 &lt;strong&gt;分裂点 &lt;span class="math"&gt;\( s\)&lt;/span&gt;  把样本划分为左右两部分&lt;/strong&gt; (类似CART的划分方式，只不过用的目标函数不同)&lt;/p&gt;
&lt;p&gt;1. 计算 &lt;strong&gt;分裂前&lt;/strong&gt; 的收益  &lt;/p&gt;
&lt;div class="math"&gt;$$
-\frac{1}{2}(\frac{(G_L+G_R)^2}{H_L+H_R+\lambda})+\gamma\\
$$&lt;/div&gt;
&lt;p&gt;2. 计算 &lt;strong&gt;分裂后&lt;/strong&gt; 的收益  &lt;/p&gt;
&lt;div class="math"&gt;$$
-\frac{1}{2}\frac{G_L^2}{H_L+\lambda}+\gamma +(-\frac{1}{2}\frac{G_L^2}{H_L+\lambda}+\gamma)\\
$$&lt;/div&gt;
&lt;p&gt;3. &lt;strong&gt;分裂前收益-分裂后收益&lt;/strong&gt; (  &lt;span class="math"&gt;\( \frac{1}{2}\)&lt;/span&gt;  这个可以舍掉)，找出 &lt;strong&gt;收益最大&lt;/strong&gt; 的分裂点  &lt;span class="math"&gt;\( s\)&lt;/span&gt;  &lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}-\gamma\\\
$$&lt;/div&gt;
&lt;p&gt;此时XGB中也可以 &lt;strong&gt;自动剪枝&lt;/strong&gt; ，剪枝主要为两种方式：一种为在 &lt;strong&gt;训练过程中通过计算分裂收益判断是否要继续向下分裂&lt;/strong&gt; (此时 &lt;strong&gt;考虑不到未来的分裂节点会带来正收益的情况&lt;/strong&gt; )，二种为生成 &lt;strong&gt;完整的树结构之后，自下而上的逆向剪枝&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;4. 最后计算出树的结构也就确定了第  &lt;span class="math"&gt;\( m\)&lt;/span&gt;  轮需要加入的树模型  &lt;span class="math"&gt;\( T_m(x;\Theta)\)&lt;/span&gt;  ，形成最终的模型，这里也可以加一个 &lt;strong&gt;&lt;em&gt;shrinkage&lt;/em&gt;&lt;/strong&gt; 的学习速率  &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;  ，来控制学习速度，给后面的学习留下一定的空间，最终的模型即是&lt;/p&gt;
&lt;div class="math"&gt;$$
H_m(x) = H_{m-1}(x) + \epsilon T_m(x; \Theta)\\
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>集成学习(随机森林 AdaBoost)</title><link href="https://reed-qu.github.io/ji-cheng-xue-xi-sui-ji-sen-lin-adaboost.html" rel="alternate"></link><published>2020-01-24T15:25:43+08:00</published><updated>2020-01-24T15:25:43+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/ji-cheng-xue-xi-sui-ji-sen-lin-adaboost.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在文章&lt;strong&gt;《&lt;a href="https://reed-qu.github.io/jue-ce-shu.html"&gt; 决策树 &lt;/a&gt;》&lt;/strong&gt;中讨论了简单决策树的构建，最后提到了决策树算法很容易引起过拟合问题，就是说当决策树贪婪的学习到“最优”的状态的时候，很容易就把数据中的一些噪声或者离群点当成正确的数据学习了进去，那么很显然会对模型的精度产生一定的影响，所以往往需要剪枝，让决策树的泛化能力更好一些，但是这里介绍另外一种更好的方法来让模型避免学习到噪声数据，从而提升模型的精度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;集成学习 Ensemble …&lt;/strong&gt;&lt;/h2&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在文章&lt;strong&gt;《&lt;a href="https://reed-qu.github.io/jue-ce-shu.html"&gt; 决策树 &lt;/a&gt;》&lt;/strong&gt;中讨论了简单决策树的构建，最后提到了决策树算法很容易引起过拟合问题，就是说当决策树贪婪的学习到“最优”的状态的时候，很容易就把数据中的一些噪声或者离群点当成正确的数据学习了进去，那么很显然会对模型的精度产生一定的影响，所以往往需要剪枝，让决策树的泛化能力更好一些，但是这里介绍另外一种更好的方法来让模型避免学习到噪声数据，从而提升模型的精度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;集成学习 Ensemble Learning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;集成学习是一种机器学习思想，可以比喻成“ &lt;strong&gt;三个臭皮匠赛过诸葛亮&lt;/strong&gt;”，就是说单个模型并不能很完美的解决某个分类或者回归问题的时候，那么就训练出多个模型，每个模型可能是相同的也可以是不同的，比如模型1是决策树，模型2是朴素贝叶斯等等，然后预测的时候将数据分别输入每个模型，最后将每个模型的输出综合起来作为该未知数据的输出，下面就简单的讨论两种比较常见的集成学习算法&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;随机森林 Random Forest（Bagging）&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;随机森林是通过集成学习的思想将多棵树合并到一起的算法，它的基本单元就是决策树，最终通过多个决策树的“ &lt;strong&gt;投票&lt;/strong&gt; ”（ &lt;strong&gt;决策树输出的众数&lt;/strong&gt;）结果作为最终的输出，随机森林理解起来主要是“ &lt;strong&gt;随机&lt;/strong&gt; ”和“ &lt;strong&gt;森林&lt;/strong&gt; ”两部分。 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;森林：很多树（决策树）放在一起，不就是形成了“森林”嘛  &lt;/li&gt;
&lt;li&gt;随机：随机地从  &lt;span class="math"&gt;\( M\)&lt;/span&gt;  特征中选取  &lt;span class="math"&gt;\( m\)&lt;/span&gt;  个特征（  &lt;span class="math"&gt;\(m &amp;lt; M\)&lt;/span&gt;  ），且随机有放回的选取样本  &lt;span class="math"&gt;\( n\)&lt;/span&gt;  （  &lt;span class="math"&gt;\( n \ll N\)&lt;/span&gt;  ，&lt;span class="math"&gt;\( N\)&lt;/span&gt;  为总样本数），单独的训练每一棵树&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/ensemble_rf.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\( N\)&lt;/span&gt;  表示所有训练数据，  &lt;span class="math"&gt;\( T_1, T_2...T_k\)&lt;/span&gt; 为抽取子集训练的不同的决策树，如下为几个 &lt;strong&gt;tips&lt;/strong&gt; :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;为什么采用有放回的方式采样&lt;/strong&gt; 。因为否则的话，每棵树的训练集都是不同的，都学习到了整体数据集的一部分规律，有点“ &lt;strong&gt;盲人摸象&lt;/strong&gt; ”的意思 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;为什么能够避免过拟合&lt;/strong&gt; 。因为过拟合大多数是因为噪声数据造成的，模型把噪声的异常数据当成数据的规律来正常学习了进去，所以导致过拟合，而采用这种随机采样的方式来做的话，图中所示的  &lt;span class="math"&gt;\( outline \space point\)&lt;/span&gt;  就只会被少数的决策树采样到，这样在最后“投票”的时候，大多数决策树学到的还是正确的规律 &lt;/li&gt;
&lt;li&gt;因为每个树的采样和训练都是 &lt;strong&gt;互相独立&lt;/strong&gt; 的，所以这里很容易把整个森林中的每棵树 &lt;strong&gt;并行训练&lt;/strong&gt; ，提高训练速度 &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\( oob \space errors \space (out \space of \space bag \space errors)\)&lt;/span&gt;  即为袋外错误率，在构建每棵树的时候都有一部分的数据没有落入该决策树的采样集中，这部分数据即为该决策树的袋外数据，这部分数据的错误率即为袋外错误率，这样也避免了模型训练完之后需要大量计算的 &lt;strong&gt;交叉验证&lt;/strong&gt; ，在模型内部即可得到该误差的无偏估计，同时随机森林中的特征重要程度也是通过  &lt;span class="math"&gt;\( oob \space errors\)&lt;/span&gt;  计算出来的 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;随机森林的通用形式即为&lt;br&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
F(x) = \frac{1}{k}\sum_{k=1}^{k}{T_k(x)}\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;随机森林中 &lt;em&gt;feature_importance&lt;/em&gt; 的计算&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;对于“森林”中的每一棵树， &lt;strong&gt;选择一定的袋外( &lt;em&gt;oob&lt;/em&gt; )数据来计算误差 &lt;/strong&gt; ，假定这里为  &lt;span class="math"&gt;\( oe1\)&lt;/span&gt;  ( &lt;em&gt;oob errors 1)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;给所有样本的 &lt;strong&gt;某一个特征&lt;/strong&gt; &lt;span class="math"&gt;\( X\)&lt;/span&gt;  &lt;strong&gt;随机添加噪声数据&lt;/strong&gt; ，再次计算误差，这里为  &lt;span class="math"&gt;\( oe2\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;假设有 &lt;strong&gt;T&lt;/strong&gt; 棵树，则该特征  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  的 &lt;strong&gt;&lt;em&gt;feature_importance&lt;/em&gt;&lt;/strong&gt; 即为  &lt;span class="math"&gt;\( \frac{1}{T}\sum_{i=1}^{T}{(oe2_i-oe1_i)}\)&lt;/span&gt;  ，这里也很好理解， &lt;strong&gt;误差相差越大，其实就是随机加入的噪声特别影响特征&lt;/strong&gt; &lt;span class="math"&gt;\( X\)&lt;/span&gt;  ，也就是说特征  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  是比较重要的&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;strong&gt;自适应增强 AdaBoost（Boosting）&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;上面说的随机森林算法中其实默认了一个情况，就是  &lt;span class="math"&gt;\( T_1, T_2...T_k\)&lt;/span&gt;  这些决策树的 &lt;strong&gt;权重是一样的&lt;/strong&gt; 都为  &lt;span class="math"&gt;\( \frac{1}{k}\)&lt;/span&gt;  ，但是其实这是不足够合理的，因为决策树总会有正确率高的和低的，更合理的做法其实是&lt;strong&gt;高的决策树权重更高&lt;/strong&gt; ， &lt;strong&gt;正确率低的决策树权重更低甚至丢掉该决策&lt;/strong&gt; 树，这样会让整个模型更加准确， &lt;strong&gt;AdaBoost&lt;/strong&gt;算法即是针对这种情况做出的优化，其核心思想是 &lt;strong&gt;针对同一个训练集训练不同的弱模型&lt;/strong&gt; ，然后把这些弱模型组合起来形成一个更强的模型，算法本身是通过&lt;strong&gt;改变数据的分布&lt;/strong&gt; 来实现的，数据本身也有权重，将错分的数据也提高权重来更关注这部分数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;算法的详细步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将样本数据集中的每一个样本数据初始化一个权重  &lt;span class="math"&gt;\( w^{(i)}\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;通过对整体样本集的学习得到第一个 &lt;strong&gt;弱模型&lt;/strong&gt; &lt;span class="math"&gt;\( G_k\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;计算该弱模型的 &lt;strong&gt;错误率&lt;/strong&gt; &lt;span class="math"&gt;\( e_k\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;计算该弱模型的 &lt;strong&gt;权重&lt;/strong&gt; &lt;span class="math"&gt;\( \alpha_k\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;更新样本数据的权重  &lt;span class="math"&gt;\( w\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;用更新后的权重数据训练第二个弱模型  &lt;span class="math"&gt;\( G_{k+1}\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;然后不断 &lt;strong&gt;循环2-6步&lt;/strong&gt; ，最终由  &lt;span class="math"&gt;\( k\)&lt;/span&gt;  个弱模型得到一个准确率更高的模型  &lt;span class="math"&gt;\( F(x)\)&lt;/span&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现有训练集  &lt;span class="math"&gt;\( D ={(x^{(1)}, y^{(1)}), \space (x^{(2)}, y^{(2)}),
\space (x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\( x^{(i)}\in R^n, y\in {-1,
+1\\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1. 对每一个数据  &lt;span class="math"&gt;\( x^{(i)}\)&lt;/span&gt;  &lt;strong&gt;初始化&lt;/strong&gt; 一个权重  &lt;span class="math"&gt;\( w^{(i)}\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
w^{(i)}=\frac{1}{N}\\
$$&lt;/div&gt;
&lt;p&gt;2. 针对  &lt;span class="math"&gt;\( k = 1, 2, ..., K\)&lt;/span&gt;  训练一个弱模型，  &lt;span class="math"&gt;\( G_k\)&lt;/span&gt;  ，这里 &lt;span class="math"&gt;\( k=1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;3. 计算错误率  &lt;span class="math"&gt;\( e_k\)&lt;/span&gt;  ，这里  &lt;span class="math"&gt;\( I\)&lt;/span&gt;  为 &lt;strong&gt;indicator&lt;/strong&gt;函数，计数为True的个数，这里实际上就是计算所有数据中 &lt;strong&gt;错误的加权比例&lt;/strong&gt; 而已&lt;/p&gt;
&lt;div class="math"&gt;$$
e_k = \sum_{i=1}^{N}{w^{(i)}I(G_k(x^{(i)})\ne y^{(i)})} \\
$$&lt;/div&gt;
&lt;p&gt;4. 通过  &lt;span class="math"&gt;\( e_k\)&lt;/span&gt;  计算  &lt;span class="math"&gt;\( G_k\)&lt;/span&gt;  的权重  &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;  ，这里是应用下面图示关系曲线来计算的，结合函数曲线其实也很容易理解，就是说该第  &lt;span class="math"&gt;\( k\)&lt;/span&gt;  个&lt;strong&gt;弱模型的错误率越低&lt;/strong&gt; ，那么这个 &lt;strong&gt;弱模型的权重就越高&lt;/strong&gt; ，这里  &lt;span class="math"&gt;\( 0.5\)&lt;/span&gt; 是个很有意思的点，如果在2分类问题中一个模型的正确率为  &lt;span class="math"&gt;\( \frac{1}{2}\)&lt;/span&gt; ，那么这个模型实则是没有任何作用的，蒙一下也能达到这个正确率，所以对于这种模型来说就是可以丢掉的，权重也就是  &lt;span class="math"&gt;\( 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/ensemble_ada_weight.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;5. 更新样本数据的权重  &lt;span class="math"&gt;\( w_k^{(i)}\Rightarrow w_{k+1}^{(i)}\)&lt;/span&gt;  ，这里 &lt;span class="math"&gt;\( k,k+1\)&lt;/span&gt;  代表第  &lt;span class="math"&gt;\( k, k+1\)&lt;/span&gt;  轮迭代&lt;/p&gt;
&lt;div class="math"&gt;$$
w_k= (w_k^{(1)}, w_k^{(2)},..., w_k^{(N)})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
w_{k+1}^{(i)} = \frac{w_k^{(i)}}{Z_k}exp(-\alpha_k y^{(i)}G_k(x^{(i)})) \\
$$&lt;/div&gt;
&lt;p&gt;这里先不考虑  &lt;span class="math"&gt;\( Z_k\)&lt;/span&gt;  因为它只是一个 &lt;strong&gt;归一化系数&lt;/strong&gt; ，最终达到的效果就是每一轮迭代之后让所有样本的权重之和为 &lt;span class="math"&gt;\( 1\)&lt;/span&gt;  ，此时有两种情况，当错 &lt;strong&gt;分的时候就会提高该错分数据的权重&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果  &lt;span class="math"&gt;\( y^{(i)} G_k(x^{(i)})=1\)&lt;/span&gt;  为分类 &lt;strong&gt;正确&lt;/strong&gt; ，那么  &lt;span class="math"&gt;\( w_k^{(i)}exp(-\alpha_k)\)&lt;/span&gt;  下降 &lt;/li&gt;
&lt;li&gt;如果  &lt;span class="math"&gt;\( y^{(i)} G_k(x^{(i)})=-1\)&lt;/span&gt;  为分类 &lt;strong&gt;错误&lt;/strong&gt; ，那么  &lt;span class="math"&gt;\( w_k^{(i)}exp(-\alpha_k)\)&lt;/span&gt;  上升 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6. 用更新权重后的数据训练第下一个弱模型，然后不断迭代最终得到一个强模型 &lt;span class="math"&gt;\( F(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
F(x^{(i)})=sign(\sum_{k=1}^{k}{\alpha_k G_k(x^{(i)})})\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;二者都是基于集成学习思想构建的一种算法，都能避免一定的过拟合问题，让模型泛化能力更好，准确率也更高 &lt;/li&gt;
&lt;li&gt;随机森林因为采样的动作，且每棵树是独立的，所以更容易 &lt;strong&gt;并行&lt;/strong&gt; ，训练效率很高，而AdaBoost则不可以，因为每一个弱模型都 &lt;strong&gt;依赖于上一个弱模型&lt;/strong&gt; ，所以无法并行训练 &lt;/li&gt;
&lt;li&gt;随机森林由于不但样本采样，数据特征也做了采样，所以对 &lt;strong&gt;特征工程&lt;/strong&gt; 要求就不是那么严格，而AdaBoost则没有采样动作 &lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>决策树</title><link href="https://reed-qu.github.io/jue-ce-shu.html" rel="alternate"></link><published>2020-01-24T15:15:30+08:00</published><updated>2020-01-24T15:15:30+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/jue-ce-shu.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;今天来研究一下分类问题中的决策树，决策树从名字就可以看出来，是一种&lt;strong&gt;树形结构&lt;/strong&gt;的决策模型。如标题图中所看见的，我们想通过是“否拥有房产”、“是否已婚”、“年收入”这3个维度，来判断该客户是否具有偿还某种贷款的能力，就可以建造一个这样的1个树形结构模型，但是这其中有两个问题：1 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;今天来研究一下分类问题中的决策树，决策树从名字就可以看出来，是一种&lt;strong&gt;树形结构&lt;/strong&gt;的决策模型。如标题图中所看见的，我们想通过是“否拥有房产”、“是否已婚”、“年收入”这3个维度，来判断该客户是否具有偿还某种贷款的能力，就可以建造一个这样的1个树形结构模型，但是这其中有两个问题：1，为什么判断顺序是图中所示的顺序；2，为什么拥有房产为“是”的时候就停止向下判断，这里简单介绍几种决策树相关的算法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;信息论 Information Theory&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;信息论目的是寻找一种可以量化的方法，来量化信息量的多少，针对这个问题我们就可以对事件  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  做出如下基本的规律总结&lt;/p&gt;
&lt;div class="math"&gt;$$
\left\{
    \begin{aligned}
        Info(X) &amp;amp; \geq 0 &amp;amp; 1 \\
        Info(X) &amp;amp; \Leftrightarrow \frac{1}{P(X)} &amp;amp; 2\\
        Info(X_1, X_2) &amp;amp; = Info(X_1) + Info(X_2) &amp;amp; 3
    \end{aligned}
\right.
$$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;事件的发生，最多是没有产生任何信息量，不可能还少了信息量，所以信息量是 &lt;strong&gt;大于等于0&lt;/strong&gt; 的 &lt;/li&gt;
&lt;li&gt;考虑这样2个事件，A: 领导说今天晚上加班；B: 领导说要给你升职加薪。很明显B事件的信息量要大于A事件，且  &lt;span class="math"&gt;\( P(A) &amp;gt; P(B)\)&lt;/span&gt;  ，所以这里认为 &lt;strong&gt;信息量与该事件的发生概率成反比&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;多个 &lt;strong&gt;独立&lt;/strong&gt; 事件的信息量为每个事件的信息量之和 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以事件  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  与该事件的信息量  &lt;span class="math"&gt;\( Info(X)\)&lt;/span&gt; 可以得出这样的等式，来满足上面的3个要求&lt;/p&gt;
&lt;div class="math"&gt;$$
Info(X) = log_2\frac{1}{P(X)} = -log_2(P(X))\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;信息熵 Information Entropy&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;信息熵也叫香农熵，是香农于1948年10月发表的论文《通信的数学理论》中提出的，在该文中，香农给出了信息熵的定义，即是信息量的数学期望&lt;/p&gt;
&lt;div class="math"&gt;$$
Entropy(X) = \sum_{X\epsilon \chi}^{}{P(X)Info(X)} = -\sum_{X\epsilon \chi}^{}{P(X)log_2(P(X))}\\
$$&lt;/div&gt;
&lt;p&gt;其中  &lt;span class="math"&gt;\( \chi\)&lt;/span&gt;  为有限个事件  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  的集合，  &lt;span class="math"&gt;\( X\)&lt;/span&gt; 是定义在  &lt;span class="math"&gt;\( \chi\)&lt;/span&gt;  上的随机变量，是随机事件 &lt;strong&gt;不确定性的度量&lt;/strong&gt; ，如果事件为连续变量，即是把 &lt;span class="math"&gt;\( \sum_{}^{}{}\)&lt;/span&gt;  变成  &lt;span class="math"&gt;\( \int_{}^{}\)&lt;/span&gt;  求曲线积分。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;信息增益 Information Gain&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;信息增益是系统从一个状态变成另一个状态之后，信息熵变小(系统 &lt;strong&gt;纯净度增大&lt;/strong&gt; 或者系统的 &lt;strong&gt;不确定性减少&lt;/strong&gt; 的程度)的增益效果量化。状态变更之前叫做&lt;strong&gt;初始状态熵&lt;/strong&gt; ，状态变更之后叫 &lt;strong&gt;条件熵&lt;/strong&gt; ，信息增益就是 &lt;strong&gt;初始状态熵 - 条件熵&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
IG = Original Entropy - \sum_{}^{}{w_iEntropy(X_i)}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( w_i\)&lt;/span&gt;  为切分之后每部分的频率，也就是加权的概念，切分的越好，各部分的信息熵越小，信息增益也就越大。比如说  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  为“明天下雨”这一随机事件，  &lt;span class="math"&gt;\( Y\)&lt;/span&gt; 为”明天阴天“这一随机条件，二者的信息熵都是可以通过事件的概率计算出来的，这里假如&lt;/p&gt;
&lt;div class="math"&gt;$$
Entropy(X) = 1
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Entropy(Y) = 0.1
$$&lt;/div&gt;
&lt;p&gt;此时信息增益为&lt;/p&gt;
&lt;div class="math"&gt;$$
Entropy(X) - Entropy(Y) = 0.9\\
$$&lt;/div&gt;
&lt;p&gt;在得知“明天阴天”这一事件后，“明天下雨”的 &lt;strong&gt;不确定性减少了0.9&lt;/strong&gt; ，所以 &lt;strong&gt;信息增益增大&lt;/strong&gt;，换句话就是说，”明天阴天“这一事件对于“明天下雨”这一事件来说是很重要的，所以决策树可以用信息增益( &lt;span class="math"&gt;\( IG/ 重要程度\)&lt;/span&gt; )来判断节点的顺序&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;基尼指数 Gini Index&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;基尼指数是一个经济学系数，最初应用在判断年收入分配公平程度上，后来也应用于其他领域，代表了 &lt;strong&gt;模型的不纯度&lt;/strong&gt; ，&lt;strong&gt;基尼指数越小，则不纯度越低，特征越好&lt;/strong&gt; ，基尼指数的定义如下&lt;/p&gt;
&lt;div class="math"&gt;$$
Gini(X) = \sum_{j}^{}{P(j|X)(1-P(j|X))} = 1-\sum_{j}^{}{P(j|X)^2}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( P(j|X)\)&lt;/span&gt;  即是针对事件  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  中，  &lt;span class="math"&gt;\( j\)&lt;/span&gt; 类的频率，若给定样本  &lt;span class="math"&gt;\( N\)&lt;/span&gt;  ，  &lt;span class="math"&gt;\( C\)&lt;/span&gt;  把  &lt;span class="math"&gt;\( N\)&lt;/span&gt;  分为 &lt;span class="math"&gt;\( C_1，C_2\)&lt;/span&gt;  两部分，则基尼系数为&lt;/p&gt;
&lt;div class="math"&gt;$$
Gini(N, C) = \sum_{i=1}^{2}{\frac{|C_i|}{|N|}Gini(C_i)}\\
$$&lt;/div&gt;
&lt;p&gt;即两部分的 &lt;strong&gt;加权&lt;/strong&gt; 基尼指数，  &lt;span class="math"&gt;\( C\)&lt;/span&gt;  &lt;strong&gt;切分的越好，系统纯度越高，系统不纯度越低，基尼指数越小&lt;/strong&gt;，这样通过不断的计算，找到最小的基尼指数即切分点，也可以遍历计算最终形成决策树&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;分类和回归树 CART Classification And Regression Tree&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CART是一个二叉树，从名字就能看出来CART可以用来处理 &lt;strong&gt;分类&lt;/strong&gt; 和 &lt;strong&gt;回归&lt;/strong&gt; 问题，构造CART的过程是 &lt;strong&gt;递归地构建二叉决策树&lt;/strong&gt; 的过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分类树&lt;/strong&gt; 通常采用Gini指数来构建树，即分别计算所有特征的Gini指数，然后用&lt;strong&gt;最小Gini指数的特征及相应的划分方式作为根节点和该特征的划分方式&lt;/strong&gt; ，然后在其他维度上重复进行此操作，最终形成分类树&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;回归树&lt;/strong&gt; 处理的是连续变量，通常采用 &lt;strong&gt;最小均方误差&lt;/strong&gt; 的方法来构建，通俗讲起来就是，在  &lt;span class="math"&gt;\( j\)&lt;/span&gt; 个维度上，找到最优的切分点  &lt;span class="math"&gt;\( s\)&lt;/span&gt;  ，使得划分出来的  &lt;span class="math"&gt;\( D_1, D_2\)&lt;/span&gt; &lt;strong&gt;两部分数据的均方误差最小&lt;/strong&gt; ，总结起来的公式如下&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    Cost &amp;amp;= \mathop{min}_{j,s}[\mathop{min}_{j, s}D_1, \mathop{min}_{j, s}D_2]\\
    &amp;amp;= \mathop{min}_{j,s}[\mathop{min}_{j, s} L_2(y^{(i)}, c_1), \mathop{min}_{j, s} L_2(y^{(i)},c_2)]
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( L_2\)&lt;/span&gt;  表示欧式距离 &lt;span class="math"&gt;\( c_1, c_2\)&lt;/span&gt;  为  &lt;span class="math"&gt;\( D_1, D_2\)&lt;/span&gt;  两部分数据的均值，一般情况下递归的过程中，有这样几种情况的时候，就可以停止分裂&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点中的 &lt;strong&gt;样本数量小于阈值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;节点中的 &lt;strong&gt;Gini指数小于阈值&lt;/strong&gt; ，则代表分类足够好 &lt;/li&gt;
&lt;li&gt;没有更多的特征可以分裂了 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回归树举例来说，针对一维数据集(多维的情况只是遍历的时候也要遍历其他维度来寻找最好切分点)，通过以下过程可以形成回归树&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  轴上遍历计算各个切分点的  &lt;span class="math"&gt;\( Cost\)&lt;/span&gt;  ，找到  &lt;span class="math"&gt;\( s_1\)&lt;/span&gt;  点 &lt;/li&gt;
&lt;li&gt;在  &lt;span class="math"&gt;\( j\)&lt;/span&gt;  轴上的  &lt;span class="math"&gt;\( s_1\)&lt;/span&gt;  点处将数据切成左右两部分(红色虚线左右) &lt;/li&gt;
&lt;li&gt;红色虚线左部分  &lt;span class="math"&gt;\( c_1\)&lt;/span&gt;  停止分裂(误差小于阈值)，右部分继续二分 &lt;/li&gt;
&lt;li&gt;在右部分遍历计算  &lt;span class="math"&gt;\( Cost\)&lt;/span&gt;  找到切分点  &lt;span class="math"&gt;\( s_2\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\( c_2, c_3\)&lt;/span&gt;  部分误差均小于阈值，所以停止分裂 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/dt_cart.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;至此回归树简历完成，形成的树形结构为&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/dt_tree.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;信息熵、基尼指数二者的图像曲线关系如下，熵之半即是  &lt;span class="math"&gt;\( Entropy/2\)&lt;/span&gt;  ，二者的图像很相似，即 &lt;span class="math"&gt;\( p=0.5\)&lt;/span&gt;  时，熵和基尼指数都 &lt;strong&gt;到达最大&lt;/strong&gt; ， &lt;strong&gt;即最不稳定&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/dt_diff.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;决策树模型简单，易于理解，但是普通的决策树构造过程是 &lt;strong&gt;贪婪&lt;/strong&gt;的，所谓贪婪就是决策树会一直分裂下去，直到没有更多的特征可以分裂了，所以对于普通的决策树来说 &lt;strong&gt;过拟合&lt;/strong&gt; 很常见，这样一来就需要 &lt;strong&gt;剪枝&lt;/strong&gt;使模型提前终止分裂( &lt;strong&gt;预剪枝&lt;/strong&gt; )或者对完整的树进行剪枝( &lt;strong&gt;后剪枝&lt;/strong&gt; )来让模型更简单、泛化能力更好，或者用 &lt;strong&gt;集成算法&lt;/strong&gt; 来避免过拟合问题&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>对数几率回归(逻辑回归)</title><link href="https://reed-qu.github.io/dui-shu-ji-lu-hui-gui-luo-ji-hui-gui.html" rel="alternate"></link><published>2020-01-24T14:55:27+08:00</published><updated>2020-01-24T14:55:27+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-24:/dui-shu-ji-lu-hui-gui-luo-ji-hui-gui.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;这回我们还是从回归问题说起，提到回归问题一般是对连续性因变量进行预测的建模方法，那么假如现在的因变量为 &lt;strong&gt;离散的2分类&lt;/strong&gt;，例如预测明天是否会下雨，预测比赛是否会赢这一类的问题，回归的方法是否可行呢？答案是当然的了，但是要改变一下方式。  思考一下，现在要做的事情是通过某种模型，要对一系列的值进行转化， &lt;strong&gt;输出0或者1&lt;/strong&gt; ，代表 &lt;strong&gt;成功或者失败 …&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;这回我们还是从回归问题说起，提到回归问题一般是对连续性因变量进行预测的建模方法，那么假如现在的因变量为 &lt;strong&gt;离散的2分类&lt;/strong&gt;，例如预测明天是否会下雨，预测比赛是否会赢这一类的问题，回归的方法是否可行呢？答案是当然的了，但是要改变一下方式。  思考一下，现在要做的事情是通过某种模型，要对一系列的值进行转化， &lt;strong&gt;输出0或者1&lt;/strong&gt; ，代表 &lt;strong&gt;成功或者失败&lt;/strong&gt;，很直观的想法是分别计算这一系列的值 &lt;strong&gt;为0和为1的概率&lt;/strong&gt; ，较大的概率自然也就是预测的结果了。  所谓“ &lt;strong&gt;逻辑回归&lt;/strong&gt; ”的叫法其实是不准确的，是 &lt;strong&gt;一种不准确的普遍叫法&lt;/strong&gt; ，一般叫做“ &lt;strong&gt;逻辑斯特回归&lt;/strong&gt; ”(音译)，或者为“&lt;strong&gt;对数几率回归&lt;/strong&gt; ”(个人认为这种叫法更合理一些，下文论证为什么)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;对数几率回归&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;按照惯例数据集为  &lt;span class="math"&gt;\( D = {(x^{(1)}, y^{(1)}), (x^{(2)},y^{(2)})...(x^{(N)}, y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\( x^{(i)}\in R^n\)&lt;/span&gt; ，要构造的函数为  &lt;span class="math"&gt;\( H_\theta(X)\)&lt;/span&gt;  。一般简单的线性回归模型  &lt;span class="math"&gt;\( H_\theta(X) = \theta^TX\)&lt;/span&gt;  假定的是  &lt;span class="math"&gt;\( X\)&lt;/span&gt; 与真实的  &lt;span class="math"&gt;\( Y\)&lt;/span&gt;  (因变量)呈 &lt;strong&gt;线性关系&lt;/strong&gt; ，但是前面也提到了，现在要做2分类问题的话可以把因变量转化成概率的形式来处理，所以这里对数几率回归则是假定  &lt;span class="math"&gt;\( X\)&lt;/span&gt;  与 &lt;span class="math"&gt;\( Y\)&lt;/span&gt;  的 &lt;strong&gt;&lt;a href="https://zh.wikipedia.org/zh-hans/%E5%8F%91%E7%94%9F%E6%AF%94"&gt;几率&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;的对数&lt;/strong&gt; 呈线性关系, 也就是与2分类的 &lt;strong&gt;二者的发生几率然后取对数&lt;/strong&gt; 呈线性关系，写出来即是&lt;/p&gt;
&lt;div class="math"&gt;$$
ln\frac{P(Y=1)}{P(Y=0)}=\theta^TX\\
$$&lt;/div&gt;
&lt;p&gt;这里  &lt;span class="math"&gt;\( P(Y=0)= 1 - P(Y=1)\)&lt;/span&gt; ，然后做下面的公式转换&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{P(Y=1)}{1-P(Y=1)} = exp(\theta^TX)\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
P(Y=1) = \frac{exp(\theta^TX)}{1+exp(\theta^TX)}\\
$$&lt;/div&gt;
&lt;p&gt;这里上下同时乘 &lt;span class="math"&gt;\( exp(-\theta^TX)\)&lt;/span&gt;  ，即得到&lt;/p&gt;
&lt;div class="math"&gt;$$
 P(Y=1)=H_\theta(X)= \frac{1}{1+exp(-\theta^TX)}\\
$$&lt;/div&gt;
&lt;p&gt;上面的公式即是 &lt;strong&gt;对数几率回归&lt;/strong&gt; 的模型，也可以看出其实就是在线性模型 &lt;span class="math"&gt;\(\theta^TX\)&lt;/span&gt; 的基础上做了特殊的变换得来的，函数曲线画出来就更直观一些&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/lr_sigmoid.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;通过函数曲线可以更直观的看出，对数几率回归实际上是在线性回归的基础上，通过一个函数把值映射到0-1之间，从而解决2分类的问题，这个函数叫做&lt;strong&gt;&lt;a href="https://zh.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid函数&lt;/a&gt;&lt;/strong&gt; 。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;损失函数&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;接下来就是要构造损失函数  &lt;span class="math"&gt;\( J(\theta)\)&lt;/span&gt;  ，普通线性回归中我们用  &lt;span class="math"&gt;\(H_\theta(X)\)&lt;/span&gt;  与  &lt;span class="math"&gt;\( Y\)&lt;/span&gt;  的 &lt;strong&gt;差的平方和 &lt;/strong&gt;来作为损失函数，这里也可以同理通过计算概率的形式来计算出最大的概率构造损失函数，可以理解为： &lt;strong&gt;让正确分类的部分(&lt;em&gt;这里既然是2分类的概率的形式，显然是呈二项分布的&lt;/em&gt; )的概率累乘尽量大，那么拟合出来的  &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  即为更好 &lt;/strong&gt;。&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = -\frac{1}{N}[\prod_{i=1}^{N}\sum_{j=0}^{1}{1(y^{(i)}=j)}\times P(y^{(i)}=j)]\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
J(\theta) =-\frac{1}{N}[\sum_{i=1}^{N}\sum_{j=0}^{1}{1(y^{(i)}=j)}\times lnP(y^{(i)}=j)]\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
J(\theta) = -\frac{1}{N}\sum_{i=1}^{N}(y^{(i)}\times ln(H_\theta(x^{(i)})) + (1-y^{(i)})\times ln(1-H_\theta(x^{(i)})))\\
$$&lt;/div&gt;
&lt;p&gt;上面的公式有几点需要解释一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;负号&lt;/strong&gt; ：损失函数一般为越小越好，所以这里取相反数 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1&lt;/strong&gt; ：此为 &lt;strong&gt;&lt;a href="https://zh.wikipedia.org/wiki/%E6%8C%87%E7%A4%BA%E5%87%BD%E6%95%B0"&gt;Indicator函数&lt;/a&gt;&lt;/strong&gt;  ，即计算预测正确的内部分的概率 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;j&lt;/strong&gt; ：代表2分类中其中的1个分类 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ln&lt;/strong&gt; ：因为是概率相乘，而概率都是小于1的数，所以为了计算方便，会把累乘通过取对数变成累加 &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;梯度下降&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;有了损失函数，就可以用梯度下降来迭代求解了，首先要计算偏导  &lt;span class="math"&gt;\( \frac{\partial
J(\theta)}{\partial \theta_j}\)&lt;/span&gt;  ，这里为了书写简便去掉  &lt;span class="math"&gt;\( (i)\)&lt;/span&gt;  角标&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\partial \theta_j}=-\frac{1}{N}\sum_{i=1}^{N}{ \frac{y}{H_\theta(x)}\times \frac{\partial H_\theta(x)}{\partial \theta_j}}+\frac{1-y}{1-H_\theta(x)}\times \frac{\partial (1-H_\theta(x))}{\partial \theta_j}\\
$$&lt;/div&gt;
&lt;p&gt;其中这里&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial H_\theta(x)}{\partial \theta_j}=\frac{\partial}{\partial \theta_j} \frac{1}{1+exp(-\theta^Tx)}=H_\theta(x)\times (1-H_\theta(x))\times x_j\\
$$&lt;/div&gt;
&lt;p&gt;同理得&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial (1-H_\theta(x))}{\partial \theta_j}=-H_\theta(x)\times (1-H_\theta(x))\times x_j
$$&lt;/div&gt;
&lt;p&gt;组合公式化简即得&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\partial \theta_j} = -\frac{1}{N}\sum_{i=1}^{N}{(y^{(i)}-H_\theta(x^{(i)})) x_j}
$$&lt;/div&gt;
&lt;p&gt;有了偏导就可以迭代更新求解  &lt;span class="math"&gt;\( \theta_j \Leftarrow \theta_j - \alpha
\frac{\partial J(\theta)}{\partial \theta_j}\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;对数几率回归的特点&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;对数几率回归&lt;/strong&gt; 是广义线性模型的一种，本质上是对线性模型通过 &lt;strong&gt;sigmoid函数&lt;/strong&gt; 的映射，所以模型清晰，可解释性较强，这也是为什么对数几率回归虽然模型简单但是仍然流行在各种实际场景中，比如 &lt;strong&gt;&lt;a href="https://zh.wikipedia.org/wiki/%E9%BB%9E%E6%93%8A%E7%8E%87"&gt;CTR&lt;/a&gt;&lt;/strong&gt;  &lt;strong&gt;点击率预估&lt;/strong&gt; 。 &lt;/li&gt;
&lt;li&gt;因为本质上还是线性回归，所以有这和线性回归一样的局限性，比如多 &lt;strong&gt;重共线性问题&lt;/strong&gt; 。 &lt;/li&gt;
&lt;li&gt;也是由于本质上是线性回归，所以也可以应用 &lt;strong&gt;L1&lt;/strong&gt; 、 &lt;strong&gt;L2正则化&lt;/strong&gt; 来处理 &lt;strong&gt;过拟合&lt;/strong&gt; 和 &lt;strong&gt;多重共线性&lt;/strong&gt; 的问题 &lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>线性回归中的正则化</title><link href="https://reed-qu.github.io/xian-xing-hui-gui-zhong-de-zheng-ze-hua.html" rel="alternate"></link><published>2020-01-22T23:21:04+08:00</published><updated>2020-01-22T23:21:04+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-22:/xian-xing-hui-gui-zhong-de-zheng-ze-hua.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;今天的话题从线性回归开始，在应对线性回归问题的时候，实质上就是训练1个函数  &lt;span class="math"&gt;\(f(X) = \theta^TX = Y\)&lt;/span&gt;，这个等式可以通过我之前的文章 &lt;a href="https://reed-qu.github.io/pu-tong-zui-xiao-er-cheng-fa.html"&gt; 最小二乘法 &lt;/a&gt; 来计算，即 &lt;span class="math"&gt;\(\theta = (X^TX …&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;今天的话题从线性回归开始，在应对线性回归问题的时候，实质上就是训练1个函数  &lt;span class="math"&gt;\(f(X) = \theta^TX = Y\)&lt;/span&gt;，这个等式可以通过我之前的文章 &lt;a href="https://reed-qu.github.io/pu-tong-zui-xiao-er-cheng-fa.html"&gt; 最小二乘法 &lt;/a&gt; 来计算，即 &lt;span class="math"&gt;\(\theta = (X^TX)^{-1}X^TY\)&lt;/span&gt;  ，但是由于最小二乘法需要计算矩阵的逆，所以有很多的限制，比如&lt;strong&gt;矩阵不可逆&lt;/strong&gt; ，又或者矩阵中有 &lt;strong&gt;多重共线性&lt;/strong&gt; 的情况，会导致计算矩阵的逆的时候行列式接近0，对数据很敏感，还有可能在训练模型的时候有&lt;strong&gt;过拟合&lt;/strong&gt;的情况出现(如下图，第3个图的曲线精确的学习到了所有的数据点，显然就是过拟合了)，下面这里通过解决实际问题，一点一点推算出来解决这些问题的2种方法：&lt;strong&gt;&lt;em&gt;L1&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;L2&lt;/em&gt;&lt;/strong&gt; 正则化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/regularization.jpg"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;L2正则化&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;先说一下&lt;strong&gt;L2正则化&lt;/strong&gt;的思想，开篇提到过，最小二乘法有很多限制和很多弊端，比如 &lt;strong&gt;多重共线性&lt;/strong&gt; 这个问题，由于需要计算矩阵的逆，所以训练出的模型系数往往会比较大_(行列式接近0)_ ，这样模型会很不稳定，所以大牛们就想啊，能不能给最小二乘法加上点 &lt;strong&gt;&lt;em&gt;惩罚&lt;/em&gt;&lt;/strong&gt;，让这个系数小一些，模型更稳定，泛化能力更好，于是就有了这个方法。&lt;/p&gt;
&lt;p&gt;L2正则化即是在最小二乘法的基础上，加1个对系数的 &lt;strong&gt;&lt;em&gt;“惩罚项”&lt;/em&gt;&lt;/strong&gt; ，为了方便计算所以加上的是 &lt;strong&gt;&lt;em&gt;L2-norm&lt;/em&gt;&lt;/strong&gt; 的平方，这时候损失函数就为&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \frac{1}{2N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})^2} + \frac{\lambda}{2}||\theta||_2^2\\
$$&lt;/div&gt;
&lt;p&gt;这里添加了一个  &lt;span class="math"&gt;\(\frac{\lambda}{2} ||\theta||_2^2\)&lt;/span&gt;  就是 &lt;strong&gt;L2&lt;/strong&gt; 正则化做的事情，是为了考虑损失的同时还要兼顾模型也就是  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  的大小，不让  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  变成1个很大的数，从而避免过拟合或者说让模型更简单，泛化能力更强，然后对  &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;  求导。&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})x_j} + \lambda \theta_j\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \theta_j^{'} &amp;amp;= \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j}\\
    &amp;amp;= \theta_j - \alpha(\frac{1}{N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})x_j} + \lambda\theta_j)\\
    &amp;amp;= \theta_j - \alpha \lambda \theta_j - \alpha(\frac{1}{N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})x_j})\\
    &amp;amp;=(1-\alpha\lambda)\theta_j - \alpha(\frac{1}{N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})x_j})
\end{aligned}\\
$$&lt;/div&gt;
&lt;p&gt;这里根据推导我们就能看出来，  &lt;span class="math"&gt;\( \alpha\frac{1}{N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})x_j}\)&lt;/span&gt;  这部分就是普通的 &lt;a href="https://reed-qu.github.io/pi-liang-ti-du-xia-jiang.html"&gt; 批量梯度下降&lt;/a&gt; ，同时  &lt;span class="math"&gt;\( \alpha\)&lt;/span&gt;  和 &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;  都是一个 &lt;strong&gt;小于1&lt;/strong&gt; 的比较小的数字，所以这里做的就是在每次迭代的时候，把 &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  再缩小一点， &lt;em&gt;“缩小多少”&lt;/em&gt; 这个因子则是由  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt; 控制的，通过调整  &lt;span class="math"&gt;\( \lambda\)&lt;/span&gt;  的大小来调整模型是更关注 &lt;strong&gt;损失&lt;/strong&gt; 还是更关注 &lt;strong&gt;模型的复杂度&lt;/strong&gt; 。&lt;/p&gt;
&lt;p&gt;从 &lt;a href="https://reed-qu.github.io/pu-tong-zui-xiao-er-cheng-fa.html"&gt; 最小二乘法 &lt;/a&gt; 的角度来思考，也可以直接计算 &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  的解析解 &lt;span class="math"&gt;\( J(\theta) = ||X\theta - Y||_2^2 +\lambda||\theta||_2^2\)&lt;/span&gt;，这里省略展开过程，可以得到&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    J(\theta) &amp;amp;= ||X\theta -Y||_2^2 +\lambda||\theta||_2^2\\
    &amp;amp;=\theta^TX^TX\theta - 2\theta^TX^TY + Y^TY \lambda \theta^T\theta \end{aligned}\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \frac{\partial J(\theta)}{\partial \theta} &amp;amp;= \frac {\partial (\theta^TX^TX\theta - 2\theta^TX^TY + Y^TY + \lambda \theta^T\theta)}{\partial \theta}\\
    &amp;amp;=2X^TX\theta - 2X^TY +\lambda \theta\\
    &amp;amp;=2(X^TX+\frac{\lambda I}{2})\theta-2X^TY
\end{aligned}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\partial \theta} =
2(X^TX+\frac{\lambda I}{2})\theta -2X^TY = 0
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\theta = (X^TX+\frac{\lambda}{2}I)^{-1}X^TY
$$&lt;/div&gt;
&lt;p&gt;这样也是可以直接计算出  &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  的，只是加上了1个 &lt;span class="math"&gt;\(\frac{\lambda}{2}I\)&lt;/span&gt;  ，这里有2点需要注意&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有很多普通最小二乘法矩阵不可逆的情况，这里加上了  &lt;span class="math"&gt;\( \frac{\lambda}{2}I\)&lt;/span&gt;  就变得可逆了，让最小二乘法更通用 &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\( X^TX\)&lt;/span&gt;  由于  &lt;span class="math"&gt;\( x_0\)&lt;/span&gt;  这项所以是  &lt;span class="math"&gt;\( (n+1)\times(n+1)\)&lt;/span&gt;  的矩阵，这里的  &lt;span class="math"&gt;\( I\)&lt;/span&gt;  也应该是  &lt;span class="math"&gt;\( (n+1)\times(n+1)\)&lt;/span&gt;  的，但是有一点不同就是这里的  &lt;span class="math"&gt;\( I\)&lt;/span&gt;  实际上是&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
\left[ \begin{array}{ccccc}
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0\\
    \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp;1
\end{array} \right]\\
$$&lt;/div&gt;
&lt;p&gt;因为这里并不需要对  &lt;span class="math"&gt;\( \theta_0\)&lt;/span&gt;  这个 &lt;strong&gt;偏置项&lt;/strong&gt; 做缩放,这种线性回归方法我们也叫做 &lt;strong&gt;岭回归(Ridge Regression)&lt;/strong&gt; ，即加了 &lt;strong&gt;L2-norm&lt;/strong&gt; 正则化的线性回归。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;L1正则化&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;这里和&lt;strong&gt;L2&lt;/strong&gt;有点不同： &lt;strong&gt;L2&lt;/strong&gt;虽然很优秀，计算出来的系数更稳定，但是多重共线性仍然存在，有没有一种类似这种添加惩罚项的办法来解决掉多重共线性的问题？其实这里很自然的就想到，把&lt;strong&gt;L2&lt;/strong&gt; 正则化中的惩罚项替换成 &lt;strong&gt;L0-norm&lt;/strong&gt; (系数中非0的个数)，这样模型就更简单了，无关的维度系数为0，即是去除掉该维度，但是&lt;strong&gt;L0-norm&lt;/strong&gt; 又有很多问题（函数非连续，很难求解），所以用 &lt;strong&gt;L1-norm&lt;/strong&gt; (系数的绝对值)来近似地取代 &lt;strong&gt;L0-norm&lt;/strong&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;L1&lt;/strong&gt; 正则化的推导公式跟 &lt;strong&gt;L2&lt;/strong&gt; 正则化的推导类似，区别主要在添加的惩罚项部分，所以为了理解这里可以这么写这个损失函数&lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta) = \frac{1}{2N}\sum_{i=1}^{N}{(H_\theta(x^{(i)})-y^{(i)})^2} +
\lambda||\theta||_1 = J_0(\theta) + J_1(\theta)\\
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\( J_0(\theta)\)&lt;/span&gt;  为一般线性回归部分，  &lt;span class="math"&gt;\(J_1(\theta)\)&lt;/span&gt;  为&lt;strong&gt;L1-norm&lt;/strong&gt; 部分 &lt;span class="math"&gt;\(\frac{\partial J(\theta)}{\partial \theta} = \lambda\times sgn(\theta)+ \frac{\partial J_0(\theta)}{\partial \theta}\)&lt;/span&gt;  ，梯度下降更新 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  的时候即是 &lt;span class="math"&gt;\(\theta_j^{'} = \theta_j - \alpha \lambda sgn(\theta_j) - \alpha\frac{\partial J_0(\theta)}{\partial \theta}\)&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其中  &lt;span class="math"&gt;\(\alpha \lambda sgn(\theta_j)\)&lt;/span&gt;  项，当  &lt;span class="math"&gt;\(\theta_j &amp;gt; 0\)&lt;/span&gt;  时，更新后  &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  变小，当  &lt;span class="math"&gt;\(\theta_j &amp;lt; 0\)&lt;/span&gt;  时，更新后  &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  变大，所以这里的效果就是让 &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  往0上靠拢，让  &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  中尽可能多的为0，也就是常说的让 &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  &lt;strong&gt;更稀疏&lt;/strong&gt; ，所以说 &lt;strong&gt;L1&lt;/strong&gt; 正则化可以 &lt;strong&gt;优化过拟合和做特征选择&lt;/strong&gt; ，因为为0的 &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  就是不重要的特征，但是这里有一个问题，当  &lt;span class="math"&gt;\( \theta_j=0\)&lt;/span&gt; 的时候怎么办？因为此时  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  是不可导的，所以粗暴一点就是按照普通最小二乘法的方式去迭代，也就是去掉 &lt;span class="math"&gt;\( \alpha \lambda sgn(\theta_j)\)&lt;/span&gt;  项或者令其为0，这样就解决了，更通用一点的是利用 &lt;a href="[https://zh.wikipedia.org/zh-hans/%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D%E6%B3%95](https://zh.wikipedia.org/zh-hans/坐标下降法)"&gt;坐标下降法&lt;/a&gt; 来求解，本质就是控制其他维度，单独调整某一个维度的  &lt;span class="math"&gt;\( \theta_j\)&lt;/span&gt;  ，确定之后再继续调整其他的 &lt;span class="math"&gt;\( \theta\)&lt;/span&gt;  ，也可以解决。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这种线性回归方法我们也叫做 &lt;strong&gt;LASSO回归(least absolute shrinkage and selection operator)&lt;/strong&gt;，即加了 &lt;strong&gt;L1-norm&lt;/strong&gt; 的惩罚项的线性回归。&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;总结 LASSO 和 Ridge 回归&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;相同点：&lt;/strong&gt; 都是在线性回归的基础上，添加了一个惩罚项，让模型更稳定更简单，泛化能力更强，避免过拟合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;不同点：LASSO&lt;/strong&gt; 是利用 &lt;strong&gt;L1-norm&lt;/strong&gt; 来逼近 &lt;strong&gt;L0-norm&lt;/strong&gt;的，因为系数可以减小到0，所以可以做特征选择，但是并不是每个点都是可导的，所以计算起来可能会并没有 &lt;strong&gt;L2-norm&lt;/strong&gt; 方便； &lt;strong&gt;Ridge&lt;/strong&gt; 是利用 &lt;strong&gt;L2-norm&lt;/strong&gt; 来使系数不那么大，同时方便计算，但是不会使系数减小到0，所以不能做特征选择，可解释性方面也没有 &lt;strong&gt;LASSO&lt;/strong&gt; 高。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>朴素贝叶斯分类</title><link href="https://reed-qu.github.io/po-su-bei-xie-si-fen-lei.html" rel="alternate"></link><published>2020-01-21T16:06:57+08:00</published><updated>2020-01-21T16:06:57+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-21:/po-su-bei-xie-si-fen-lei.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在机器学习中， &lt;strong&gt;朴素贝叶斯&lt;/strong&gt; （Naive Bayes）是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单 &lt;strong&gt;概率分类器&lt;/strong&gt;。朴素贝叶斯自20世纪50年代已广泛研究。在20世纪60年代初就以另外一个名称引入到文本信息检索界中，并仍然是 &lt;strong&gt;文本分类&lt;/strong&gt;的一种热门（基准）方法 …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在机器学习中， &lt;strong&gt;朴素贝叶斯&lt;/strong&gt; （Naive Bayes）是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单 &lt;strong&gt;概率分类器&lt;/strong&gt;。朴素贝叶斯自20世纪50年代已广泛研究。在20世纪60年代初就以另外一个名称引入到文本信息检索界中，并仍然是 &lt;strong&gt;文本分类&lt;/strong&gt;的一种热门（基准）方法，文本分类是以词频为特征判断文件所属类别或其他（如垃圾邮件、合法性、体育或政治等等）的问题。通过适当的预处理，它可以与这个领域更先进的方法（包括支持向量机）相竞争。它在自动医疗诊断中也有应用。参考&lt;a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"&gt;朴素贝叶斯分类器
&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;strong&gt;贝叶斯定理&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;要理解贝叶斯定理其实比较简单，实际上就是常说的 “ &lt;em&gt;条件概率&lt;/em&gt; ”，所谓条件概率，就是假定事件A发生的基础上，事件B发生的概率，用 &lt;span class="math"&gt;\( P(B|A)\)&lt;/span&gt;  来表示，叫做“ &lt;em&gt;给定A时，B的概率&lt;/em&gt; ”。&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image-small" src="/images/nb_prob_condition.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;考虑样本空间  &lt;span class="math"&gt;\( \Omega\)&lt;/span&gt;  中有2个事件A和B，其中 &lt;strong&gt;相交&lt;/strong&gt; 的部分为  &lt;span class="math"&gt;\( A\cap B\)&lt;/span&gt;  ，这里可以很容易计算出贝叶斯公式&lt;/p&gt;
&lt;div class="math"&gt;$$
P(B|A)=\frac{P(A\cap B)}{P(A)} = \frac{P(A|B)\cdot P(B)}{P(A)}\\
$$&lt;/div&gt;
&lt;p&gt;一般的，这里  &lt;span class="math"&gt;\( P(A|B)\)&lt;/span&gt;  叫做 &lt;strong&gt;似然概率，&lt;/strong&gt; &lt;span class="math"&gt;\( P(B)\)&lt;/span&gt;  叫做&lt;strong&gt;先验概率&lt;/strong&gt; ，  &lt;span class="math"&gt;\( P(A)\)&lt;/span&gt;  是1个归一化系数，跟我们要计算的”给定  &lt;span class="math"&gt;\( A\)&lt;/span&gt;  时 &lt;span class="math"&gt;\( B\)&lt;/span&gt;  的概率“中的  &lt;span class="math"&gt;\( B\)&lt;/span&gt;  (即我们主要关注的点)是无关的&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;公式推导&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; . 实际情况中我们遇到的问题一般是，给定1个数据样本，根据朴素贝叶斯对之做出分类，即  &lt;span class="math"&gt;\(P(C |X)\)&lt;/span&gt;  ”给定数据样本  &lt;span class="math"&gt;\(X\)&lt;/span&gt;  ，分类为  &lt;span class="math"&gt;\(C\)&lt;/span&gt;  的概率“，根据贝叶斯公式有&lt;/p&gt;
&lt;div class="math"&gt;$$
P(C|X) = \frac{P(X|C)P(C)}{P(X)} = \frac{似然概率\times先验概率}{P(X)}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; . 这里  &lt;span class="math"&gt;\(X\)&lt;/span&gt;  要转化成跟  &lt;span class="math"&gt;\(C\)&lt;/span&gt;  相关的话就可以这样做，即&lt;/p&gt;
&lt;div class="math"&gt;$$
P(X) = P(X|C_1)\times P(X|C_2)\times ... \times P(X|C_j)\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; . 所以这里朴素贝叶斯公式可以转化成如下（  &lt;span class="math"&gt;\(C \in (C_1, C_2...C_j)\)&lt;/span&gt;  个分类）&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        P(C_j|X) &amp;amp;=\frac{P(X|C_j)P(C_j)}{P(X)}\\
        &amp;amp;=\frac{P(X|C_j)P(C_j)}{P(X|C_1)\times P(X|C_2)\times \cdots \times P(X|C_j)}   \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; . 重点来了，这里的  &lt;span class="math"&gt;\(X\)&lt;/span&gt;  是1个n维的数据  &lt;span class="math"&gt;\(x_1,x_2,...,x_n\)&lt;/span&gt;  ，这里讨论的是朴素贝叶斯，之所以称为朴素，就是这里有1个前提：  &lt;span class="math"&gt;\(X\)&lt;/span&gt;  中的每个维度都是&lt;strong&gt;相互独立&lt;/strong&gt; 的（ &lt;em&gt;虽然现实生活中往往并不是这样，但是即便是我们假定的有一些偏差，但是还是可以解决很多问题，也说明了朴素贝叶斯的强大&lt;/em&gt; ），所以这里我们主要专注研究  &lt;span class="math"&gt;\(P(X|C_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        P(X|C_j) &amp;amp;= P(x_1,x_2 \cdots x_n|C_j)\\
        &amp;amp;= P(x_1|C_j) \times P(x_2|C_j) \times \cdots \times P(x_n|C_j)\\
        &amp;amp;=\prod_{i=1}^{n}P(x_i|C_j)
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; . 最后就可以整理成&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        P(C|X) &amp;amp;=\frac{P(X|C)P(C)}{P(X)}\\
        &amp;amp;=\frac{P(C) \prod_{i=1}^{n}P(x_i|C)}{P(X)}
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;这其中 &lt;span class="math"&gt;\(P(C)\times \prod_{i=1}^{n}P(x_i|C)\)&lt;/span&gt;  为主要关注的点，与  &lt;span class="math"&gt;\(P(C|X)\)&lt;/span&gt;  &lt;strong&gt;正相关&lt;/strong&gt; ，下面通过处理实际问题的例子具体感受一下朴素贝叶斯的神奇&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;“是否要去看电影”问题&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;比如说我喜欢去看电影，但是电影并不是想看就随时能看的，所以这里就要通过一些信息来做判断，判断我”是否要去看电影“&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里有我近10次看电影的记录，大概总结起来主要有4个维度来影响我是否要去看电影，分别是：&lt;strong&gt;&lt;em&gt;是否周末，天气好坏，地点远近，电影评分的高低&lt;/em&gt;&lt;/strong&gt;，然后现在的情况是：&lt;strong&gt;是周末，天气坏，地点远，评分高的电影&lt;/strong&gt; ，但是我现在不想权衡到底是看还是不看，这时候朴素贝叶斯就能很好的解决我的问题。&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/nb_example.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;根据朴素贝叶斯公式，这里写的通俗一些，其实就是数数，比如  &lt;span class="math"&gt;\( P(看)\)&lt;/span&gt;  就是在所有样本中” &lt;em&gt;看&lt;/em&gt; “占的比例，&lt;span class="math"&gt;\( P(是周末|看)\)&lt;/span&gt;  就是在看的5天中有3天是周末，所以概率就为  &lt;span class="math"&gt;\( \frac{3}{5}\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        P(看|X) &amp;amp;\Rightarrow P(看|是周末，天气坏，地点远，评分高)\\
        &amp;amp;=P(看)\times P(是周末|看) \times P(天气坏|看) \times P(地点远|看)\times P(评分高|看)\\
        &amp;amp;=\frac{5}{10}\times \frac{4}{5} \times \frac{2}{5} \times\frac{2}{5} \times \frac{1}{5} =0.0128    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        P(不看|X) &amp;amp;\Rightarrow P(不看|是周末，天气坏，地点远，评分高)\\
        &amp;amp;=P(不看)\times P(是周末|不看) \times P(天气坏|不看) \times P(地点远|不看) \times P(评分高|不看)\\ 
        &amp;amp;=\frac{5}{10}\times \frac{1}{10000} \times \frac{3}{5} \times \frac{3}{5} \times \frac{2}{5}\\
        &amp;amp;=0.0000072
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;这里有2点需要说明注意一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意到每个公式的第一个“ &lt;strong&gt;&lt;em&gt;等于号&lt;/em&gt;&lt;/strong&gt; ”实际上不是 “ &lt;span class="math"&gt;\( =\)&lt;/span&gt;  ”，而是“  &lt;span class="math"&gt;\(\Rightarrow\)&lt;/span&gt;  ”，这里代表 &lt;strong&gt;正相关&lt;/strong&gt; ，因为这里并不是严格的等于概率的，还需要加上  &lt;span class="math"&gt;\( P(X)\)&lt;/span&gt; 这个归一化系数，可以直接比较他们的大小，因为分母是不变的嘛，但是并不代表计算的是概率，实际上做归一化之后就是所求概率了即&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    P(看|X) &amp;amp;= \frac{0.0128}{(0.0128+0.0000072)} = 0.99\\
    P(不看|X) &amp;amp;= 0.01
\end{aligned}
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;注意到  &lt;span class="math"&gt;\( P(是周末|不看)\)&lt;/span&gt;  “ &lt;em&gt;不看电影的时候，是周末&lt;/em&gt; ”，这种情况在我们样本中并没有发生所以概率就是0，但是如果其中之一为0，概率连乘之后会导致整个计算结果也为0，所以这里需要做一个 &lt;strong&gt;平滑&lt;/strong&gt;，这种用1个很小的数代替0的办法一般叫做 &lt;strong&gt;拉普拉斯平滑&lt;/strong&gt; ，通常有2种方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指定一个很小的数字比如  &lt;span class="math"&gt;\( 1 \times 10^{-4}\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;在“ &lt;strong&gt;&lt;em&gt;垃圾邮件分类器&lt;/em&gt;&lt;/strong&gt; ”的时候也可能会“ &lt;strong&gt;&lt;em&gt;分子+1&lt;/em&gt;&lt;/strong&gt; ”和“ &lt;strong&gt;&lt;em&gt;分母+总词数&lt;/em&gt;&lt;/strong&gt; ”这种办法，但是思想是一样的 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;算法简单，可解释性强&lt;/li&gt;
&lt;li&gt;分类效率较高，只是进行普通的数字运算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于 “ &lt;strong&gt;&lt;em&gt;朴素&lt;/em&gt;&lt;/strong&gt; ” 的强假设，分类的准确率会有所牺牲，比如在做 “ &lt;strong&gt;&lt;em&gt;垃圾邮件分类&lt;/em&gt;&lt;/strong&gt; ” 的时候，我们知道，一封邮件中的词不是相互独立的，比如说 “ &lt;strong&gt;&lt;em&gt;我想喝xxx&lt;/em&gt;&lt;/strong&gt; ”，这里 &lt;strong&gt;&lt;em&gt;xxx&lt;/em&gt;&lt;/strong&gt; 大概率是一种饮品，而不是 “ &lt;strong&gt;&lt;em&gt;电脑&lt;/em&gt;&lt;/strong&gt; ”, “ &lt;strong&gt;&lt;em&gt;手机&lt;/em&gt;&lt;/strong&gt; ” 等其他词汇，这时就可能会导致分类错误的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>普通最小二乘法</title><link href="https://reed-qu.github.io/pu-tong-zui-xiao-er-cheng-fa.html" rel="alternate"></link><published>2020-01-21T15:20:30+08:00</published><updated>2020-01-21T15:20:30+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-21:/pu-tong-zui-xiao-er-cheng-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/pi-liang-ti-du-xia-jiang.html"&gt; 批量梯度下降 &lt;/a&gt;中讨论了，如何利用梯度下降的方式，如何一步一步寻找到损失函数的最小值，得到最佳拟合的  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，这里我们继续讨论线性拟合问题，这次尝试用&lt;a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/zh-
hans/%25E6%259C%2580%25E5%25B0%258F%25E4%25BA%258C%25E4%25B9%2598%25E6%25B3%2595"&gt; 最小二乘法 &lt;/a&gt;直接求解  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，就是说我们不用从山顶寻找梯度一步一步的往最低点走，某种情况下可以直接计算出  &lt;span class="math"&gt;\(\theta …&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在 &lt;a href="https://reed-qu.github.io/pi-liang-ti-du-xia-jiang.html"&gt; 批量梯度下降 &lt;/a&gt;中讨论了，如何利用梯度下降的方式，如何一步一步寻找到损失函数的最小值，得到最佳拟合的  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，这里我们继续讨论线性拟合问题，这次尝试用&lt;a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/zh-
hans/%25E6%259C%2580%25E5%25B0%258F%25E4%25BA%258C%25E4%25B9%2598%25E6%25B3%2595"&gt; 最小二乘法 &lt;/a&gt;直接求解  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，就是说我们不用从山顶寻找梯度一步一步的往最低点走，某种情况下可以直接计算出  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;strong&gt;普通最小二乘法 &lt;em&gt;(Ordinary least squares)&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;最小二乘法&lt;/strong&gt; （又称 &lt;strong&gt;最小平方法&lt;/strong&gt;）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配，利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小，下面一步一步推导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; . 最初我们是有一组训练数据 &lt;span class="math"&gt;\(D = {(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)})...(x^{(N)},y^{(N)})}\)&lt;/span&gt;  ，其中  &lt;span class="math"&gt;\(x^{(i)}\in R^n\)&lt;/span&gt; 我们想要拟合的曲线为  &lt;span class="math"&gt;\(H(\theta) = \theta^TX = \theta_0X_0 + \theta_1X_1 + \cdots +\theta_nX_n\)&lt;/span&gt;  ，这里  &lt;span class="math"&gt;\(X_0 = 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; . 现在我们用矩阵的形式来表示  &lt;span class="math"&gt;\(H(\theta)\)&lt;/span&gt;  ，我们有 &lt;span class="math"&gt;\(N\)&lt;/span&gt; 个样本数据，每个数据维度是 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 维&lt;/p&gt;
&lt;div class="math"&gt;$$
\left[
    \begin{array}{c}
        \theta_0X_0^{(1)} + \theta_1X_1^{(1)} + \cdots + \theta_nX_n^{(1)} = y^{(1)}\\
        \theta_0X_0^{(2)} + \theta_1X_1^{(2)} + \cdots + \theta_nX_n^{(2)} = y^{(2)}\\
        \vdots \\
        \theta_0X_0^{(N)} + \theta_1X_1^{(N)}+ \cdots + \theta_nX_n^{(N)} = y^{(N)}
    \end{array}
\right]\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; . 这里把上面的公式组合拆分成3部分  &lt;span class="math"&gt;\(X\)&lt;/span&gt;  数据集，  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 参数，  &lt;span class="math"&gt;\(Y\)&lt;/span&gt;  目标值&lt;/p&gt;
&lt;div class="math"&gt;$$
X = 
\left[ 
    \begin{array}{ccc}
        1 &amp;amp; x_1^{(1)} &amp;amp; \cdots &amp;amp; x_n^{(1)}\\
        1 &amp;amp; x_1^{(2)} &amp;amp; \cdots &amp;amp; x_n^{(2)}\\
        \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\
        1 &amp;amp; x_1^{(N)} &amp;amp; \cdots &amp;amp; x_n^{(N)}
    \end{array}
\right]\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\theta = 
\left[
    \begin{array}{c}
        \theta_1\\
        \theta_2\\
        \vdots\\
        \theta_n
    \end{array}
\right]\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Y = 
\left[ 
    \begin{array}{c}
    y_1\\
    y_2\\
    \vdots\\
    y_n
    \end{array}
\right]\\
$$&lt;/div&gt;
&lt;p&gt;这时等式就可以写成  &lt;span class="math"&gt;\(X\theta = Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; . 我们想要通过这N个等式来求解  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  ，当然这里不一定会有 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  会使所有等式都成立，这时候就还是需要一个损失函数来判断取  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的时候，误差是否为最小，这里就和梯度下降是一个思路，用误差平方和来代表损失，但是这里是用矩阵的形式来表示，矩阵的 &lt;strong&gt;&lt;em&gt;L-2&lt;/em&gt;&lt;/strong&gt; 范数即表示误差平方和&lt;/p&gt;
&lt;div class="math"&gt;$$
CostFunction = J(\theta) = ||X\theta - Y||_2^2\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; . 这时候就不需要用梯度下降的方法去一步一步计算，还是可以根据人物下山的例子想象一下，梯度下降是人物以 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;  为步长，走一步计算一下梯度再走下一步，而这里我们更像是人物开了天眼，一次性从所有的数据中找到最优解  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt; . 首先来展开  &lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        J(\theta) = ||X\theta - Y||_2^2 
        &amp;amp;= (X\theta - Y)^T(X\theta - Y)\\
        &amp;amp; = (\theta^TX^T - Y^T)(X\theta -Y)\\
        &amp;amp;=\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta + Y^TY\\
        &amp;amp;=\theta^TX^TX\theta - 2\theta^TX^TY + Y^TY
    \end{aligned}
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;上式 &lt;span class="math"&gt;\(\theta^TX^TY\)&lt;/span&gt;  是一个标量，因为 &lt;span class="math"&gt;\(\theta^T \in 1\times (n+1)\)&lt;/span&gt; , &lt;span class="math"&gt;\(X^T \in (n+1)\times N\)&lt;/span&gt; , &lt;span class="math"&gt;\(Y
\in N\times1\)&lt;/span&gt;，所以根据矩阵乘法的特点，这里乘出来之后会是1个标量，&lt;span class="math"&gt;\(Y^TX\theta\)&lt;/span&gt; 同理，所以二者可以合并成 &lt;span class="math"&gt;\(2\theta^TX^TY\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 7&lt;/strong&gt; . 在梯度下降中，我们一步一步的计算梯度，一直走到最低点也就是导数为0的点，这里可以直接对损失函数求导，令导数=0即可&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
    \begin{aligned}
        \frac{\partial J(\theta)}{\partial \theta}
        &amp;amp;= \frac {\partial (\theta^TX^TX\theta - 2\theta^TX^TY + Y^TY)}{\partial \theta}\\
        &amp;amp;=\frac{\partial(\theta^TX^TX\theta)}{\partial \theta} - 2X^TY + 0
    \end{aligned}
\end{equation}\\
$$&lt;/div&gt;
&lt;p&gt;此时注意到  &lt;span class="math"&gt;\(X^TX\)&lt;/span&gt;  是一个  &lt;span class="math"&gt;\((n+1)\)&lt;/span&gt;  维的方阵，这时候根据公式&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial (X^TBX)}{\partial X} = (B + B^T) X\\
$$&lt;/div&gt;
&lt;p&gt;如上矩阵相关的公式，都可以在这个&lt;a href="https://link.zhihu.com/?target=http%3A//117.128.6.34/cache/www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf%3Fich_args2%3D466-11132213008203_0b26da0a993385059ef3d90549e99ec6_10001002_9c896128dec5f1d0943b518939a83798_efb0b935aca0c3bc1c9af15cf2a1260c"&gt; The Matrix Cookbook
&lt;/a&gt;中查找到，所以可以推导出  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  用 &lt;span class="math"&gt;\(X\)&lt;/span&gt;  数据集和  &lt;span class="math"&gt;\(Y\)&lt;/span&gt;  目标集来表示&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \frac{\partial(\theta^TX^TX\theta)}{\partial \theta} &amp;amp;= (X^TX +X^TX) \theta = 2X^TX\theta\\
    \frac{\partial J(\theta)}{\partial \theta} &amp;amp;= 2X^TX\theta - 2X^TY = 0\\
    \theta &amp;amp;= (X^TX)^{-1}X^TY
\end{aligned}\\
$$&lt;/div&gt;
&lt;h3&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;至此已经计算出线性回归中 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的值，也就是说，我们不必要用梯度下降的方式去迭代收敛到最小值，可以直接根据公式得到 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 清晰的解析解，但是最小二乘法并不能解决所有情况，有以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;通过公式  &lt;span class="math"&gt;\(\theta = (X^TX)^{-1}X^TY\)&lt;/span&gt;  发现对在数据集基础上做 &lt;strong&gt;逆运算，&lt;/strong&gt;计算矩阵的逆需要大量的计算，虽然现在计算机能力都很强，但是那也架不住，动辄上百万的数据量和上万的维度，所以首先 &lt;strong&gt;数据量过大&lt;/strong&gt;不适合用最小二乘法，反观梯度下降就没有这个问题( 一步一步迭代嘛 )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还是逆运算，并不是所有的矩阵都是可逆的，如果数据集中有 &lt;strong&gt;多重共线性&lt;/strong&gt; 的问题( &lt;strong&gt;矩阵行列式=0&lt;/strong&gt; )，这里求逆就行不通了，即便是可逆，多重共线性也会让结果很不准确，但是梯度下降还是可以一步一步收敛。当然了也可以通过比如降维，求伪逆，损失函数加上个惩罚项( 后面会讨论&lt;strong&gt;&lt;em&gt;LASSO&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;Ridge&lt;/em&gt;&lt;/strong&gt; )等办法对数据集进行处理，转化成可逆矩阵。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>批量梯度下降</title><link href="https://reed-qu.github.io/pi-liang-ti-du-xia-jiang.html" rel="alternate"></link><published>2020-01-21T14:58:57+08:00</published><updated>2020-01-21T14:58:57+08:00</updated><author><name>reed</name></author><id>tag:reed-qu.github.io,2020-01-21:/pi-liang-ti-du-xia-jiang.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在机器学习问题中，基本上都是基于特定的损失函数，来迭代优化这个函数值，既然是损失函数，代表的是损失的多少，所以通常是寻找最小值，梯度下降法即是一种不断寻找函数极值点的方法，通常用“人物下山”来举例子，考虑你现在山上的某一点，你想要快速下山，这个过程有3点是需要确定的，才能完成这个动作&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;我一步能迈多远 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;在机器学习问题中，基本上都是基于特定的损失函数，来迭代优化这个函数值，既然是损失函数，代表的是损失的多少，所以通常是寻找最小值，梯度下降法即是一种不断寻找函数极值点的方法，通常用“人物下山”来举例子，考虑你现在山上的某一点，你想要快速下山，这个过程有3点是需要确定的，才能完成这个动作&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;我一步能迈多远，步子越大当然速度就越快&lt;/li&gt;
&lt;li&gt;方向，当然要往下走，而且是越陡越好&lt;/li&gt;
&lt;li&gt;走到”谷底“的时候就停止&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;梯度&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;梯度即函数收敛过程中，变化最快的方向，这样才不会走偏而“绕远”，这里从微积分开始说起&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;微积分&lt;/strong&gt;，可以理解为函数某点上的斜率，下面大概举几个例子 &lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{d(x^3)}{dx}=3x^2\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{d(x^n)}{dx}=nx^{n-1}\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial x}(x^2y^2)=2xy^2\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;梯度&lt;/strong&gt;，就是多变量微分，最终的梯度也就是1个向量, 代表在各个方向上的分量  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  在这时为一个向量 &lt;/p&gt;
&lt;div class="math"&gt;$$
J(\theta)=2 - (-\theta_1 + 2\theta_2 + 0.5\theta_3)\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\frac{\partial J(\theta)}{\partial \theta}=[\frac{\partial
J}{\partial \theta_1}, \frac{\partial J}{\partial \theta_2}, \frac{\partial
J}{\partial \theta_3}]=[1, -2, -0.5]\\
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;梯度&lt;/strong&gt;就是那个下降最快的方向，具体的梯度下降的公式下面解释，这里举一个&lt;strong&gt;线性回归&lt;/strong&gt;的例子&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;线性回归&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;我们有若干数据，想要拟合出一条合理的回归线，我们就可以用梯度下降不断迭代求出这条直线的2个参数  &lt;span class="math"&gt;\(\theta_0,\theta_1\)&lt;/span&gt;  ，即  &lt;span class="math"&gt;\(f(x) = \theta_0 + \theta_1 x_1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/gd_line_reg.png"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这里 &lt;strong&gt;红叉表示样本点&lt;/strong&gt; ，现在我们想拟合出1条 &lt;strong&gt;最合理的直线&lt;/strong&gt; (绿色)来代表随机变量的趋势，这时候就可以用梯度下降来解决这个问题 &lt;/li&gt;
&lt;li&gt;思考一个问题，就是我们如何用数学的方式来判断某条直线就是我们想要的直线，显然这条直线不能离样本点太远，否则效果就太差了，要“雨露均沾”的距离样本点足够近才足够好 &lt;/li&gt;
&lt;li&gt;这里用 &lt;strong&gt;距离误差平方和&lt;/strong&gt; 的方式表示，也就是当前的直线到样本点的距离的平方的和，我们想要优化的就是 &lt;strong&gt;样本点到直线的距离的平方和为最小，&lt;/strong&gt; 这时候我们认为该条直线就是我们所要的，下面通过这个线性回归的例子来一步一步解释梯度下降 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; . 随机给一个  &lt;span class="math"&gt;\(\theta_0, \theta_1\)&lt;/span&gt;
，这里就是橙色这条线，当然也可以是其他的值，我们假设的函数叫做  &lt;span class="math"&gt;\(H(\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; . 计算样本点到橙色线的距离，这个误差项我们叫做 &lt;strong&gt;损失值&lt;/strong&gt; , 误差函数叫 &lt;strong&gt;损失函数&lt;/strong&gt; 即为各点到
&lt;span class="math"&gt;\(H(\theta)\)&lt;/span&gt;  的距离平方和&lt;/p&gt;
&lt;div class="math"&gt;$$
CostFunction = J(\theta) =
\frac{1}{2N}\sum_{i=1}^{N}(H_\theta(x^{(i)}) - y^{(i)})^2\\
$$&lt;/div&gt;
&lt;p&gt;这里  &lt;span class="math"&gt;\(x^{(i)}, y^{(i)}\)&lt;/span&gt;  为第 &lt;strong&gt;i&lt;/strong&gt; 个坐标点(x, y);  &lt;span class="math"&gt;\(\frac{1}{2}\)&lt;/span&gt;  则为一种数学上的 &lt;strong&gt;方便计算&lt;/strong&gt; 的系数，使得求偏导数的时候与平方项能约掉，对优化问题并不产生影响&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; . 我们想要求得的就是上面的损失函数的最低点，这里就用到梯度下降的算法，计算偏导数&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\partial \theta} = [\frac{\partial
J(\theta)}{\partial \theta_0}, \frac{\partial J(\theta)}{\partial
\theta_1}...\frac{\partial J(\theta)}{\partial \theta_j}]\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\frac{\partial J(\theta)}{\theta_j} =
\frac{1}{N}\sum_{i=1}^{N}(H_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\theta_j = \theta_j^{'} - \alpha \frac{\partial
J(\theta)}{\partial \theta_j}\\
$$&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;  代表 &lt;strong&gt;步长&lt;/strong&gt; , 即是每次迭代的速度,  &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;  的大小设置是1个经验值，太大容易错过最低点直接一步迈到另外一边的山腰上，太小容易走得太慢，走半天还没走到山底 &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  是一个向量，所以  &lt;span class="math"&gt;\(\theta_j\)&lt;/span&gt;  要 &lt;strong&gt;同时更新&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;?&lt;/strong&gt; 为什么要向梯度的负方向移动，从下图很容易看出来，最低点的右边梯度&amp;gt;0则向左移动，反之向右移动，即总是向梯度的反方向移动 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img class="image-process-article-image" src="/images/gd.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; . 如上图展示，我们要做的就是从  &lt;span class="math"&gt;\(A\)&lt;/span&gt;  点一步一步走到最低点，得到 &lt;strong&gt;损失函数&lt;/strong&gt;
的最小值，我们就认为，此时的  &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;  为最优状态&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;举个例子&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;假设我们有一个  &lt;span class="math"&gt;\(J(\theta) = \theta_0^2 + \theta_1^2\)&lt;/span&gt;  , 
那么&lt;span class="math"&gt;\(\frac{\partial J(\theta)}{\partial \theta} = 2\theta_0 + 2\theta_1\)&lt;/span&gt;  ，
这里  &lt;span class="math"&gt;\(\alpha=0.1\)&lt;/span&gt;  ，初始化  &lt;span class="math"&gt;\( \theta^{(0)} = [1, 3]\)&lt;/span&gt;  ，
我们已知这个函数的最小值为  &lt;span class="math"&gt;\([0, 0]\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    \theta^{(1)} &amp;amp;= [1, 3] - 0.1 \times [2\times1, 2\times3] = [1, 3] - [0.2, 0.6] = [0.8, 2.4]\\
    \theta^{(2)} &amp;amp;= [0.8, 2.4]-0.1\times[2\times0.8, 2\times2.4]=[0.8, 2.4]-[0.16, 0.48]=[0.64, 1.92]\\
    \theta^{(3)} &amp;amp;= [0.64, 1.92] - 0.1 \times [2\times0.64, 2\times1.92] = [0.64, 1.92] - [0.128, 0.384] = [0.512, 1.536]\\
    &amp;amp;\vdots\\
    \theta^{(100)} &amp;amp;= [1.63e^{-10}, 4.89e^{-10}]
\end{aligned}\\
$$&lt;/div&gt;
&lt;h2&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;损失函数的优化方法有很多，即便是梯度下降也有很多更好的实现方法，比如工程上更常用的&lt;strong&gt;小批量梯度下降&lt;/strong&gt;或者&lt;strong&gt;随机梯度下降&lt;/strong&gt;，本文讨论是最基本的&lt;strong&gt;批量梯度下降&lt;/strong&gt;，即每”下降“一次都要用全量的数据来计算，在实际生产环境中这个数据量往往是很大的，这么做很影响算法的性能，所以提出了小批量梯度下降或者随机梯度下降，思想就是用比较少的数据来计算梯度，降低计算量，但是算法核心是不变的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;😛&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="machine learning"></category></entry></feed>